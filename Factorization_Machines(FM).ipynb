{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 제5장 Factorization Machines(FM)\n",
    "\n",
    "FM은 사용자와 아이템의 다양한 특성을 모델화함으로써 이러한 예측의 성능을 높이려는 방볍  \n",
    "앞에서 설명했던 Matrix Factorization(MF)은 사용자의 취향과 아이템의 특성을 나타내는 특성값(feature)을 k개로 요약해서 추출하고, 이를 이용해 각 사용자의 각 아이템의 선호도를 예측하는 방식  \n",
    "그런데 사용자의 취향과 아이템의 특성뿐 아니라 예측에 도움을 줄 수 있는 다른 변수가 존재할 수 있다.  \n",
    "ex) 사용자 : 나이,성별,지역,인구 통계변수 / 아이템 : 감독,출연배우,장르  \n",
    "이러한 다양한 변수를 종합해서 요인화(factorization)해 주는 방법이 FM  "
   ],
   "id": "58ea059187fd8209"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.1 FM의 표준식\n",
    "\n",
    "FM의 기본 아이디어는 모든 변수와 그 변수들 간의 상호작용(interaction)을 고려해서 평점을 예측하는 것  \n",
    "$\\hat{y}(\\mathbf{x}) = w_0 + \\sum_{i=1}^{n} w_i x_i + \\sum_{i=1}^{n} \\sum_{j=i+1}^{n} \\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle x_i x_j$  \n",
    "- $\\hat{y}(\\mathbf{x})$ : 예측값\n",
    "- $w_0$ : 전역 bias (global bias)\n",
    "- $w_i$ : 특성 $i$의 1차 가중치\n",
    "- $x_i$ :  특성 $i$의 입력값\n",
    "- $\\mathbf{v}_i \\in \\mathbb{R}^k$ : 특성 $i$에 대한 k-차원 잠재 벡터 (latent vector)\n",
    "- $\\langle v_i,v_j \\rangle$ : 잠재 벡터간의 내적\n",
    "- n : 입력 변수의 수\n",
    "- k : 잠재요인의 수  \n",
    "\n",
    "<br>\n",
    "\n",
    "여기서 v는 MF의 P 혹은 Q행렬처럼 각 변수를 k개의 특성값(feature value)으로 표현하는 잠재요인행렬로 생각하면 된다. 즉 변수가 n개이고 특성값이 K개이므로 nxk 행렬이다.  \n",
    "하나의 변수인 $x_i$에 대해서 하나의 편향값($w_i$)과 k개의 특성값(features)을 갖게 된다.  \n",
    "만일 x가 각 사용자와 아이템 각각을 나타내는 One-hot Encoding이고 사용자와 아이템 외에 다른 변수가 사용되지 않는다면 위 식은 결국 MF의 식과 같아진다.  \n",
    "즉 **FM은 MF에 다른 변수를 추가할 수 있도록 좀 더 일반화한 모델**이다"
   ],
   "id": "5ae7469e5082a414"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.2 FM 식의 변형\n",
    "\n",
    "앞의 식은 변수 x의 모든 가능한 두 개의 조합으로 구성되어 있기 때문에 변수의 수가 늘어남에 따라 계산의 복잡도가 기하급수적으로 커진다.  \n",
    "앞의 식의 경우 변수의 수 n에 대한 복잡도를 나타내는 함수는 $O(kn^2)$이 된다  \n",
    "![img.png](image/img.png)  \n",
    "\n",
    "최종식  \n",
    "$\\hat{y}(\\mathbf{x}) = w_0 + \\sum_{i=1}^{n} w_i x_i + \\frac{1}{2} \\sum_{f=1}^{k} \\left[ \\left( \\sum_{i=1}^{n} v_{i,f} x_i \\right)^2 - \\sum_{i=1}^{n} v_{i,f}^2 x_i^2 \\right]$  \n",
    "\n",
    "초기식은 복잡도가 $O(kn^2)$으로서 변수의 수의 제곱에 비례해서 증가하는 데 비해 최종식의 복잡도는 $O(kn)$으로서 변수의 수에 선형적으로 비례해서 증가하기 때문에 문제가 커져도 효율적으로 계산이 가능하다."
   ],
   "id": "d422a2d5af428a50"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.3 FM의 학습\n",
    "\n",
    " \n",
    "$v'_{i,f} = v_{i,f} + \\alpha e (x_i \\sum_{j=1}^{n} v_{j,f}x_j - v_{j,f}x_i^2)$ ------ 식(1)\n",
    "\n",
    "1. 비용함수(예를 들면 RMSE)를 설정한다.\n",
    "2. $w_0, w, v$를 초기화한다.\n",
    "3. 주어진 $w_0, w, v$에 따라 비용함수를 계산한다.\n",
    "4. 식(1)의 update rule에 따라 $w,v$를 업데이트한다.\n",
    "5. 비용함수가 더 이상 개선되지 않을 때까지 3,4를 반복한다."
   ],
   "id": "ddce7909d4c59d5e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.4 FM의 데이터 변형\n",
    "\n",
    "데이터를 갖는 원소는 행 하나에 2개(사용자 1개와 아이템 1개)이므로 데이터를 갖는 원소의 비율이 0.018%밖에 안되는 희박행렬(sparse matrix)이 된다.  \n",
    "이러한 희박한 행렬은 원래 형태의 완전한 행렬(full matrix)로 처리하면 매우 비효율적이다.  \n",
    "그래서 값이 0이 아닌 x만 골라서 계산하는 것이 훨씬 효율적인다.  \n",
    "이와 같이 희박한 행렬을 효율적으로 계산하는 방식은 **행렬의 원소 중 0이 아닌 값을 갖는 원소만 골라서 그 원소의 인덱스만 저장해서 처리**하는 것이다."
   ],
   "id": "e3fdfa4634a2d267"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5.5 Python으로 FM의 구현  \n",
    "\n",
    "### One-hot encoding"
   ],
   "id": "e28292445693ab32"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-04T07:47:03.970928Z",
     "start_time": "2025-01-04T07:46:59.217192Z"
    }
   },
   "source": [
    "# 데이터를 읽어서 위에서 설명한 sparse matrix 형태로 변형하는 코드\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the u.data file into a dataframe\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('data/u.data', sep='\\t', names=r_cols, encoding='latin-1') \n",
    "\n",
    "# User encoding\n",
    "user_dict = {}\n",
    "for i in set(ratings['user_id']): # 사용자 ID의 중복을 제거하여 고유한 사용자 ID의 집합을 생성\n",
    "    user_dict[i] = len(user_dict)\n",
    "n_user = len(user_dict)\n",
    "\n",
    "# Item encoding\n",
    "item_dict = {}\n",
    "start_point = n_user\n",
    "for i in set(ratings['movie_id']):\n",
    "    item_dict[i] = start_point + len(item_dict)\n",
    "n_item = len(item_dict)\n",
    "start_point += n_item\n",
    "num_x = start_point\n",
    "ratings = shuffle(ratings,random_state=1)\n",
    "\n",
    "# Generate X data\n",
    "data = [] # 변수 x의 값을 [인덱스,값]의 형태로 저장\n",
    "y = [] # 평점 데이터\n",
    "w0 = np.mean(ratings['rating']) # 전체 편향값\n",
    "\n",
    "for i in range(len(ratings)):\n",
    "    case = ratings.iloc[i] # 하나의 행(하나의 평점)\n",
    "    x_index = []\n",
    "    x_value = []\n",
    "    x_index.append(user_dict[case['user_id']]) # user id encoding\n",
    "    x_value.append(1)\n",
    "    x_index.append(item_dict[case['movie_id']]) # Movie id encoding\n",
    "    x_value.append(1)\n",
    "    data.append([x_index,x_value])\n",
    "    y.append(case['rating']-w0)\n",
    "    if (i%10000)==0:\n",
    "        print('Encoding',i,'cases...')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 0 cases...\n",
      "Encoding 10000 cases...\n",
      "Encoding 20000 cases...\n",
      "Encoding 30000 cases...\n",
      "Encoding 40000 cases...\n",
      "Encoding 50000 cases...\n",
      "Encoding 60000 cases...\n",
      "Encoding 70000 cases...\n",
      "Encoding 80000 cases...\n",
      "Encoding 90000 cases...\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T13:11:37.241275Z",
     "start_time": "2025-01-03T12:56:19.590878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def RMSE(y_true,y_pred):\n",
    "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred))**2))\n",
    "\n",
    "# FM을 구현하는 클래스\n",
    "class FM():\n",
    "    def __init__(self,N,K,data,y,alpha,beta,train_ratio = 0.75,\n",
    "                 iterations=100,tolerance=0.005,l2_reg=True,verbose=True):\n",
    "        self.K=K # latent feature의 수\n",
    "        self.N=N # 변수 x의 수\n",
    "        self.n_case = len(data)\n",
    "        self.alpha = alpha # 학습률\n",
    "        self.beta = beta # 정규화 계수\n",
    "        self.iterations = iterations # 반복횟수\n",
    "        self.tolerance = tolerance # 반복을 중단하는 RMSE의 기준인 tolerance\n",
    "        self.l2_reg = l2_reg # 정규화를 할지 여부를 나타내는 값\n",
    "        self.verbose = verbose # 학습 상황을 표시할지 나타내는 값\n",
    "        \n",
    "        # 변수의 편향을 나타내는 w벡터 초기화\n",
    "        self.w = np.random.normal(scale=1./self.N,size = (self.N))\n",
    "        # 잠재요인 행렬 v 초기화\n",
    "        self.v = np.random.normal(scale=1./self.K,size = (self.N,self.K))\n",
    "        \n",
    "        # Train/Test 분리\n",
    "        cutoff = int(train_ratio*len(data))\n",
    "        self.train_x = data[:cutoff]\n",
    "        self.train_y = y[:cutoff]\n",
    "        self.test_x = data[cutoff:]\n",
    "        self.test_y = y[cutoff:]\n",
    "    \n",
    "    # Training 하면서 RMSE 계산\n",
    "    def test(self):\n",
    "        # SGD를 iterations 숫자만큼 진행\n",
    "        best_RMSE = 10000\n",
    "        best_iteration = 0\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            # SGD & Train RMSE 계산\n",
    "            rmse1 = self.sgd(self.train_x,self.train_y)\n",
    "            # Test RMSE 계산\n",
    "            rmse2 = self.test_rmse(self.test_x,self.test_y)\n",
    "            training_process.append([i,rmse1,rmse2])\n",
    "            \n",
    "            if self.verbose:\n",
    "                if(i+1)%10==0:\n",
    "                    print(\"Iteration: %d ; Train RMSE = %.6f ; Test RMSE = %.6f\" % (i+1,rmse1,rmse2))\n",
    "            \n",
    "            if best_RMSE>rmse2:\n",
    "                best_RMSE = rmse2\n",
    "                best_iteration = i\n",
    "            # RMSE가 정해진 tolerance보다 더 악화되었으면 학습을 중단\n",
    "            elif(rmse2-best_RMSE) > self.tolerance: break\n",
    "            \n",
    "        print(best_iteration,best_RMSE)\n",
    "        return training_process\n",
    "    \n",
    "    # w,v 업데이트를 위한 Stochastic gradient descent\n",
    "    def sgd(self,x_data,y_data):\n",
    "        y_pred = []\n",
    "        for data,y in zip(x_data,y_data):\n",
    "            x_idx = data[0] # x의 인덱스\n",
    "            x_0 = np.array(data[1]) # 해당 x의 값\n",
    "            x_1 = x_0.reshape(-1,1) # x의 값을 2차원으로 변형 (2차원인 v행렬과 연산을 위해서)\n",
    "            \n",
    "            # 편향값 계산\n",
    "            bias_score = np.sum(self.w[x_idx]*x_0)\n",
    "            \n",
    "            # score 계산\n",
    "            vx = self.v[x_idx] * (x_1) # v matrix * x\n",
    "            sum_vx = np.sum(vx,axis = 0) # sigma(vx)\n",
    "            sum_vx_2 = np.sum(vx*vx,axis = 0) # (v matrix * x)의 제곱\n",
    "            latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2) # FM 변형식\n",
    "            \n",
    "            # 예측값 계산\n",
    "            y_hat = bias_score + latent_score\n",
    "            y_pred.append(y_hat)\n",
    "            error = y - y_hat\n",
    "            \n",
    "            # w,v 업데이트\n",
    "            if self.l2_reg: # 정규화하는 경우의 업데이트\n",
    "                self.w[x_idx] += error * self.alpha * (x_0 - self.beta * self.w[x_idx])\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx*x_1) - self.beta * self.v[x_idx])\n",
    "            else: # 정규화하지 않는 경우 (update rule)\n",
    "                self.w[x_idx] += error * self.alpha * x_0\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx*x_1))\n",
    "        return RMSE(y_data,y_pred)\n",
    "    \n",
    "    def test_rmse(self,x_data,y_data):\n",
    "        y_pred = []\n",
    "        for data,y in zip(x_data,y_data):\n",
    "            y_hat = self.predict(data[0],data[1])\n",
    "            y_pred.append(y_hat)\n",
    "        return RMSE(y_data,y_pred)\n",
    "        \n",
    "    # 데이터 중 하나의 행에 대한 예측값을 계산하는 함수\n",
    "    # 위의 sgd() 함수에서 계산하는것과 동일\n",
    "    def predict(self,idx,x):\n",
    "        x_0 = np.array(x)\n",
    "        x_1 = x_0.reshape(-1,1)\n",
    "        \n",
    "        # 편향값 계산\n",
    "        bias_score = np.sum(self.w[idx]*x_0)\n",
    "        \n",
    "        # score 계산\n",
    "        vx = self.v[idx] * (x_1)\n",
    "        sum_vx = np.sum(vx,axis = 0)\n",
    "        sum_vx_2 = np.sum(vx*vx,axis = 0)\n",
    "        latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "        \n",
    "        # 예측값 계산\n",
    "        y_hat = bias_score + latent_score\n",
    "        return y_hat\n",
    "\n",
    "K = 350\n",
    "fm1 = FM(num_x,K,data,y,alpha=0.0014,beta=0.075,train_ratio=0.75,iterations=200,tolerance=0.005,l2_reg=True,verbose=True)\n",
    "result = fm1.test()"
   ],
   "id": "2270bb5c6565d3b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 0.955978 ; Test RMSE = 0.972694\n",
      "Iteration: 20 ; Train RMSE = 0.934231 ; Test RMSE = 0.957412\n",
      "Iteration: 30 ; Train RMSE = 0.925390 ; Test RMSE = 0.951437\n",
      "Iteration: 40 ; Train RMSE = 0.920600 ; Test RMSE = 0.948411\n",
      "Iteration: 50 ; Train RMSE = 0.917489 ; Test RMSE = 0.946656\n",
      "Iteration: 60 ; Train RMSE = 0.914981 ; Test RMSE = 0.945472\n",
      "Iteration: 70 ; Train RMSE = 0.912252 ; Test RMSE = 0.944415\n",
      "Iteration: 80 ; Train RMSE = 0.908225 ; Test RMSE = 0.943000\n",
      "Iteration: 90 ; Train RMSE = 0.901185 ; Test RMSE = 0.940517\n",
      "Iteration: 100 ; Train RMSE = 0.889110 ; Test RMSE = 0.936260\n",
      "Iteration: 110 ; Train RMSE = 0.871445 ; Test RMSE = 0.930533\n",
      "Iteration: 120 ; Train RMSE = 0.849404 ; Test RMSE = 0.924704\n",
      "Iteration: 130 ; Train RMSE = 0.823355 ; Test RMSE = 0.919572\n",
      "Iteration: 140 ; Train RMSE = 0.792668 ; Test RMSE = 0.915391\n",
      "Iteration: 150 ; Train RMSE = 0.757225 ; Test RMSE = 0.912528\n",
      "Iteration: 160 ; Train RMSE = 0.717933 ; Test RMSE = 0.911315\n",
      "Iteration: 170 ; Train RMSE = 0.676186 ; Test RMSE = 0.911799\n",
      "Iteration: 180 ; Train RMSE = 0.633335 ; Test RMSE = 0.913752\n",
      "161 0.9112805864144597\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 추가 데이터 사용\n",
    "\n",
    "사용자의 occupation, gender, age  \n",
    "영화의 genre 사용"
   ],
   "id": "b1baf125974cae4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T10:41:19.050896Z",
     "start_time": "2025-01-04T10:41:05.341099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the u.user file into a dataframe\n",
    "u_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\n",
    "users = pd.read_csv('data/u.user', sep='|', names=u_cols, encoding='latin-1')\n",
    "\n",
    "# Load the u.item file into a dataframe\n",
    "i_cols = ['movie_id', 'title', 'release date', 'video release date', 'IMDB URL', \n",
    "          'unknown', 'Action', 'Adventure', 'Animation', 'Children\\'s', 'Comedy', \n",
    "          'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n",
    "          'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "movies = pd.read_csv('data/u.item', sep='|', names=i_cols, encoding='latin-1')\n",
    "\n",
    "# Load the u.data file into a dataframe\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('data/u.data', sep='\\t', names=r_cols, encoding='latin-1')\n",
    "\n",
    "# User encoding\n",
    "user_dict = {}\n",
    "for i in set(users['user_id']):\n",
    "    user_dict[i] = len(user_dict)\n",
    "n_user = len(user_dict)\n",
    "\n",
    "# Item encoding\n",
    "item_dict = {}\n",
    "start_point = n_user\n",
    "for i in set(movies['movie_id']):\n",
    "    item_dict[i] = start_point + len(item_dict)\n",
    "n_item = len(item_dict)\n",
    "start_point += n_item\n",
    "\n",
    "# Occupation encoding\n",
    "occ_dict = {}\n",
    "for i in set(users['occupation']):\n",
    "    occ_dict[i] = start_point + len(occ_dict)\n",
    "n_occ = len(occ_dict)\n",
    "start_point += n_occ\n",
    "\n",
    "# Gender encoding\n",
    "gender_dict = {}\n",
    "for i in set(users['sex']):\n",
    "    gender_dict[i] = start_point + len(gender_dict)\n",
    "n_gender = len(gender_dict)\n",
    "start_point += n_gender\n",
    "\n",
    "# Genre encoding\n",
    "genre_dict = {}\n",
    "genre = ['unknown', 'Action', 'Adventure', 'Animation', 'Children\\'s', 'Comedy', \n",
    "          'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', \n",
    "          'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "for i in genre:\n",
    "    genre_dict[i] = start_point + len(genre_dict)\n",
    "n_genre = len(genre_dict)\n",
    "start_point += n_genre\n",
    "\n",
    "# Age encoding\n",
    "age_index = start_point\n",
    "start_point += 1\n",
    "num_x = start_point\n",
    "\n",
    "# Merge data\n",
    "movies = movies.drop(['title', 'release date', 'video release date', 'IMDB URL'], axis=1)\n",
    "users = users.drop(['zip_code'], axis=1)\n",
    "ratings = ratings.drop(['timestamp'], axis=1)\n",
    "x = pd.merge(ratings, movies, how='outer', on='movie_id') # outer 옵션을 줘서 movie_id 기준으로 병합\n",
    "x = pd.merge(x, users, how='outer', on='user_id') # outer 옵션을 줘서 user_id 기준으로 병합\n",
    "x = shuffle(x,random_state=1)\n",
    "\n",
    "# Generate X data\n",
    "data = []\n",
    "y = []\n",
    "age_mean = np.mean(x['age'])\n",
    "age_std = np.std(x['age'])\n",
    "w0 = np.mean(x['rating'])\n",
    "for i in range(len(x)):\n",
    "    case = x.iloc[i]\n",
    "    x_index = []\n",
    "    x_value = []\n",
    "    x_index.append(user_dict[case['user_id']]) # 해당 사용자 index 저장 \n",
    "    x_value.append(1)\n",
    "    x_index.append(item_dict[case['movie_id']])\n",
    "    x_value.append(1)\n",
    "    x_index.append(occ_dict[case['occupation']])\n",
    "    x_value.append(1)\n",
    "    x_index.append(gender_dict[case['sex']])\n",
    "    x_value.append(1)\n",
    "    \n",
    "    # 장르는 복수의 변수가 있으므로 loop를 돌려서 해당되는 장르 모두에 대해 기록\n",
    "    for j in genre:\n",
    "        if case[j] == 1: # 해당 영화의 장르가 j 일때\n",
    "            x_index.append(genre_dict[j])\n",
    "            x_value.append(1)\n",
    "    \n",
    "    x_index.append(age_index)\n",
    "    x_value.append((case['age'] - age_mean)/age_std) # 나이는 연속값이므로 정규화한 값을 x_value에 기록\n",
    "    data.append([x_index,x_value])\n",
    "    y.append(case['rating']-w0)\n",
    "    if (i%10000) == 0:\n",
    "        print('Encoding ',i,' cases...')"
   ],
   "id": "831c9101ec40ccc5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding  0  cases...\n",
      "Encoding  10000  cases...\n",
      "Encoding  20000  cases...\n",
      "Encoding  30000  cases...\n",
      "Encoding  40000  cases...\n",
      "Encoding  50000  cases...\n",
      "Encoding  60000  cases...\n",
      "Encoding  70000  cases...\n",
      "Encoding  80000  cases...\n",
      "Encoding  90000  cases...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T11:29:26.965466Z",
     "start_time": "2025-01-04T10:43:27.706850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터 coding 부분 외에 나머지 코드는 위의 코드와 동일\n",
    "def RMSE(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred))**2))\n",
    "\n",
    "class FM():\n",
    "    def __init__(self, N, K, data, y, alpha, beta, train_ratio=0.75, iterations=100, tolerance=0.005, l2_reg=True, verbose=True):\n",
    "        self.K = K          # Number of latent factors\n",
    "        self.N = N          # Number of x (variables)\n",
    "        self.n_cases = len(data)            # N of observations\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.l2_reg = l2_reg\n",
    "        self.tolerance = tolerance\n",
    "        self.verbose = verbose\n",
    "        # w 초기화\n",
    "        self.w = np.random.normal(scale=1./self.N, size=(self.N))\n",
    "        # v 초기화\n",
    "        self.v = np.random.normal(scale=1./self.K, size=(self.N, self.K))\n",
    "        # Train/Test 분리\n",
    "        cutoff = int(train_ratio * len(data))\n",
    "        self.train_x = data[:cutoff]\n",
    "        self.test_x = data[cutoff:]\n",
    "        self.train_y = y[:cutoff]\n",
    "        self.test_y = y[cutoff:]\n",
    "\n",
    "    def test(self):                                     # Training 하면서 RMSE 계산 \n",
    "        # SGD를 iterations 숫자만큼 수행\n",
    "        best_RMSE = 10000\n",
    "        best_iteration = 0\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            rmse1 = self.sgd(self.train_x, self.train_y)        # SGD & Train RMSE 계산\n",
    "            rmse2 = self.test_rmse(self.test_x, self.test_y)    # Test RMSE 계산     \n",
    "            training_process.append((i, rmse1, rmse2))\n",
    "            if self.verbose:\n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(\"Iteration: %d ; Train RMSE = %.6f ; Test RMSE = %.6f\" % (i+1, rmse1, rmse2))\n",
    "            if best_RMSE > rmse2:                       # New best record\n",
    "                best_RMSE = rmse2\n",
    "                best_iteration = i\n",
    "            elif (rmse2 - best_RMSE) > self.tolerance:  # RMSE is increasing over tolerance\n",
    "                break\n",
    "        print(best_iteration, best_RMSE)\n",
    "        return training_process\n",
    "        \n",
    "    # w, v 업데이트를 위한 Stochastic gradient descent \n",
    "    def sgd(self, x_data, y_data):\n",
    "        y_pred = []\n",
    "        for data, y in zip(x_data, y_data):\n",
    "            x_idx = data[0]\n",
    "            x_0 = np.array(data[1])     # xi axis=0 [1, 2, 3]\n",
    "            x_1 = x_0.reshape(-1, 1)    # xi axis=1 [[1], [2], [3]]\n",
    "    \n",
    "            # biases\n",
    "            bias_score = np.sum(self.w[x_idx] * x_0)\n",
    "            \n",
    "            # score 계산\n",
    "            vx = self.v[x_idx] * (x_1)          # v matrix * x\n",
    "            sum_vx = np.sum(vx, axis=0)         # sigma(vx)\n",
    "            sum_vx_2 = np.sum(vx * vx, axis=0)  # ( v matrix * x )의 제곱\n",
    "            latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "\n",
    "            # 예측값 계산\n",
    "            y_hat = bias_score + latent_score\n",
    "            y_pred.append(y_hat)\n",
    "            error = y - y_hat\n",
    "            # w, v 업데이트\n",
    "            if self.l2_reg:     # regularization이 있는 경우\n",
    "                self.w[x_idx] += error * self.alpha * (x_0 - self.beta * self.w[x_idx])\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx * x_1) - self.beta * self.v[x_idx])\n",
    "            else:               # regularization이 없는 경우\n",
    "                self.w[x_idx] += error * self.alpha * x_0\n",
    "                self.v[x_idx] += error * self.alpha * ((x_1) * sum(vx) - (vx * x_1))\n",
    "        return RMSE(y_data, y_pred)\n",
    "            \n",
    "    def test_rmse(self, x_data, y_data):\n",
    "        y_pred = []\n",
    "        for data , y in zip(x_data, y_data):\n",
    "            y_hat = self.predict(data[0], data[1])\n",
    "            y_pred.append(y_hat)\n",
    "        return RMSE(y_data, y_pred)\n",
    "\n",
    "    def predict(self, idx, x):\n",
    "        x_0 = np.array(x)\n",
    "        x_1 = x_0.reshape(-1, 1)\n",
    "\n",
    "        # biases\n",
    "        bias_score = np.sum(self.w[idx] * x_0)\n",
    "\n",
    "        # score 계산\n",
    "        vx = self.v[idx] * (x_1)\n",
    "        sum_vx = np.sum(vx, axis=0)\n",
    "        sum_vx_2 = np.sum(vx * vx, axis=0)\n",
    "        latent_score = 0.5 * np.sum(np.square(sum_vx) - sum_vx_2)\n",
    "\n",
    "        # 예측값 계산\n",
    "        y_hat = bias_score + latent_score\n",
    "        return y_hat\n",
    "\n",
    "K = 200\n",
    "fm1 = FM(num_x, K, data, y, alpha=0.00005, beta=0.002, train_ratio=0.75, iterations=300, tolerance=0.0001, l2_reg=True, verbose=True)\n",
    "result = fm1.test()"
   ],
   "id": "acdd89dc8aa0402d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 1.077594 ; Test RMSE = 1.080650\n",
      "Iteration: 20 ; Train RMSE = 1.053571 ; Test RMSE = 1.058320\n",
      "Iteration: 30 ; Train RMSE = 1.027617 ; Test RMSE = 1.034751\n",
      "Iteration: 40 ; Train RMSE = 1.000409 ; Test RMSE = 1.011228\n",
      "Iteration: 50 ; Train RMSE = 0.976034 ; Test RMSE = 0.991259\n",
      "Iteration: 60 ; Train RMSE = 0.957009 ; Test RMSE = 0.976550\n",
      "Iteration: 70 ; Train RMSE = 0.942962 ; Test RMSE = 0.966320\n",
      "Iteration: 80 ; Train RMSE = 0.932428 ; Test RMSE = 0.959145\n",
      "Iteration: 90 ; Train RMSE = 0.924296 ; Test RMSE = 0.954037\n",
      "Iteration: 100 ; Train RMSE = 0.917853 ; Test RMSE = 0.950361\n",
      "Iteration: 110 ; Train RMSE = 0.912619 ; Test RMSE = 0.947676\n",
      "Iteration: 120 ; Train RMSE = 0.908250 ; Test RMSE = 0.945671\n",
      "Iteration: 130 ; Train RMSE = 0.904502 ; Test RMSE = 0.944133\n",
      "Iteration: 140 ; Train RMSE = 0.901203 ; Test RMSE = 0.942916\n",
      "Iteration: 150 ; Train RMSE = 0.898228 ; Test RMSE = 0.941926\n",
      "Iteration: 160 ; Train RMSE = 0.895492 ; Test RMSE = 0.941101\n",
      "Iteration: 170 ; Train RMSE = 0.892935 ; Test RMSE = 0.940400\n",
      "Iteration: 180 ; Train RMSE = 0.890515 ; Test RMSE = 0.939796\n",
      "Iteration: 190 ; Train RMSE = 0.888205 ; Test RMSE = 0.939272\n",
      "Iteration: 200 ; Train RMSE = 0.885986 ; Test RMSE = 0.938818\n",
      "Iteration: 210 ; Train RMSE = 0.883846 ; Test RMSE = 0.938428\n",
      "Iteration: 220 ; Train RMSE = 0.881778 ; Test RMSE = 0.938098\n",
      "Iteration: 230 ; Train RMSE = 0.879778 ; Test RMSE = 0.937824\n",
      "Iteration: 240 ; Train RMSE = 0.877843 ; Test RMSE = 0.937603\n",
      "Iteration: 250 ; Train RMSE = 0.875969 ; Test RMSE = 0.937434\n",
      "Iteration: 260 ; Train RMSE = 0.874156 ; Test RMSE = 0.937313\n",
      "Iteration: 270 ; Train RMSE = 0.872401 ; Test RMSE = 0.937238\n",
      "Iteration: 280 ; Train RMSE = 0.870702 ; Test RMSE = 0.937204\n",
      "Iteration: 290 ; Train RMSE = 0.869057 ; Test RMSE = 0.937208\n",
      "Iteration: 300 ; Train RMSE = 0.867461 ; Test RMSE = 0.937246\n",
      "283 0.9372011440268232\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
