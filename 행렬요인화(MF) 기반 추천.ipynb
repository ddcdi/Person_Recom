{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 제4장 Matrix Factorization(MF) 기반 추천\n",
    "\n",
    "**메모리 기반 알고리즘** : 추천을 위한 데이터를 모두 메모리에 가지고 있으면서 추천이 필요할 때마다 이데이터를 사용해서 계산을 해서 추천하는 방식  \n",
    "   - ex) CF\n",
    "\n",
    "<br>\n",
    "\n",
    "**모델 기반 추천** : 추천을 위한 모델을 구성한 후에 이 모델만 저장하고, 실제 추천을 할 때에는 이 모델을 사용해서 추천을 하는 방식  \n",
    "   - ex) MF, Deep-Learning 방식의 추천도 데이터\n",
    "\n",
    "<br>\n",
    "\n",
    "메모리 기반 추천은 모든 데이터를 메모리에 저장하고 있기 때문에 원래 데이터를 충실하게 사용하는 장점이 있지만 대량의 데이터를 다뤄야 하는 상용 사이트에서는 계산시간이 너무 오래 걸린다는 단점이 있다  \n",
    "이에 비해 모델 기반 추천 방식은 원래 데이터는 모형을 만드는 데만 사용하고 일단 모델이 만들어진면 원래 데이터는 사용하지 않기 때문에 대규모 상용 사이트에서 필요한 빠른 반응이 가능하지만 모델을 만드는 과정에서 많은 계산이 필요하다는 단점이 있다.  \n",
    "일반적으로 메모리 기반 추천은 개별 사용자의 데이터에 집중하는 데 비해, 모델 기반 추천은 전체 사용자의 평가 패턴으로부터 모델을 구성하기 때문에 데이터가 가지고 있는 약한 신호 (weak signal)도 더 잘 잡아내는 장점이 있다.  \n",
    "  - 약한신호 : 개별 사용자의 행동분석에서는 잘 드러나지 않는 패턴\n"
   ],
   "id": "d93580ae1a73efca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.1 Matrix Factorization(MF) 방식의 원리\n",
    "\n",
    "<br>\n",
    "\n",
    "행렬요인화는 평가 데이터, 즉 (사용자 x 아이템)으로 구성된 하나의 행렬을 2개의 행렬로 분해하는 방법\n",
    "\n",
    "\n",
    "$R \\approx P \\times Q^T = \\hat{R} $\n",
    "- R : Rating matrix\n",
    "- P : User latent matrix(사용자 잠재요인행렬)\n",
    "- Q : Item latent matrix(아이템 잠재요인행렬)\n",
    "\n",
    "<br>\n",
    "\n",
    "MF 방식은 이 R행렬을 사용자행렬(P)과 아이템행렬(Q)로 쪼개어 분석하는 것  \n",
    "- P는 (M x K), Q는 (N x K)  \n",
    "- 여기서 $\\hat{R}$은 R의 예측치이며 $\\hat{R}$이 최대한 R에 가까운 값을 가지도록 하는 P와 Q를 구하면 그것이 바로 추천을 위한 모델이 된다\n",
    "- P는 각 사용자의 특성을 나타내는 K개 요인의 값으로 이루어진 행렬, Q는 각 아이템의 특성을 나타내는 K개의 요인의 값으로 이루어진 행렬\n",
    "- P와 Q행렬에서 공통인 K개의 요인이 있는데, 이를 잠재요인(latent factor)이라고 부른다.\n",
    "- 즉 사용자와 아이템의 특성을 K개의 잠재요인을 사용해서 분석하는 모델이라고 한다\n",
    "\n"
   ],
   "id": "272ef704506e9366"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.2 SGD(Stochastic Gradient Descent)를 사용한 MF 알고리즘\n",
    "\n",
    "<br>\n",
    "\n",
    "주어진 (사용자x아이템)의 평점행렬인 R로부터 P와Q를 분해하는 알고리즘\n",
    "1. 잠재요인의 개수인 K를 정한다. K는 경험에 의해 직관적으로 정해도 되고 다양한 K를 비교하면서 최적의 수를 정해도 된다.\n",
    "2. 주어진 K에 따라 P(MxK)와 Q(NxK)행렬을 만들고 초기화한다. 맨 처음에는 P,Q 행렬을 임의의 수로 채우는 것이 보통이다.\n",
    "3. 주어진 P,Q 행렬을 사용해서 예측 평점 $\\hat{R}(=P \\times Q^T)$을 구한다\n",
    "4. R에 있는 실제 평점에 대해서 예측 평점 $\\hat{R}$의 예측과 비교해서 오차를 구하고, 이 오차를 줄이기 위해서 P,Q값을 수정한다\n",
    "5. 전체 오차가 미리 정해진 기준값 이하가 되거나 미리 정해진 반복 횟수에 도달할 때까지 3번으로 돌아것 반복한다\n",
    "<br>\n",
    "\n",
    "여기서 핵심은 4번에서 예측 오차를 줄이기 위해서 P,Q를 어떻게 수정하는가이다  \n",
    "가장 일반적인 방법은 기계학습에서 많이 사용되는 SGD(Stochastic Gradient Descent) 방법을 적용하는 것이다 \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "$L = \\frac{1}{2} \\sum_{(i,j) \\in \\Omega} \\left( R_{ij} - P_i Q_j^\\top \\right)^2 + \\frac{\\lambda}{2} \\left( \\|P\\|_F^2 + \\|Q\\|_F^2 \\right)$\n",
    "\n",
    "![확률적 경사하강법](image/이미지_4-1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "![정규화항](image/이미지_4-2.png)\n",
    "\n",
    "- b : 전체 평균\n",
    "- $bu_i$ : 전체 평균을 제거한 후 사용자 i의 평가경향(사용자 i의 평균과 전체 평균의 차이)\n",
    "- $bd_j$ : 전체 평균을 제거한 후 아이템 j의 평가경향(아이템 j의 평균과 천체 평균의 차이)"
   ],
   "id": "b69710b40a899057"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.3 SGD를 사용한 MF 기본 알고리즘\n",
   "id": "c4fdcff35032bd27"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-28T10:07:37.834633Z",
     "start_time": "2024-11-28T10:06:58.074263Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv('data/u.data',sep='\\t',names=r_cols,encoding='latin-1')\n",
    "ratings = ratings[['user_id', 'movie_id', 'rating']].astype(int)\n",
    "\n",
    "# MF class\n",
    "class MF():\n",
    "    def __init__(self,ratings,k,alpha,beta,iterations,verbose=True):\n",
    "        self.R = np.array(ratings)\n",
    "        self.num_users,self.num_items = np.shape(self.R)\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.verbose =verbose # SGD의 중간 학습과정을 출력할 것인가\n",
    "        \n",
    "    # Root Mean Squared Error(RMSE) 계산\n",
    "    def rmse(self):\n",
    "        # R에서 평점이 있는(0이 아닌) 요소의 인덱스를 가져온다\n",
    "        xs,ys = self.R.nonzero()\n",
    "        self.predictions = []\n",
    "        self.errors = []\n",
    "        for x,y in zip(xs,ys):\n",
    "            prediction = self.get_prediction(x,y)\n",
    "            self.predictions.append(prediction)\n",
    "            self.errors.append(self.R[x,y]-prediction)\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        self.errors = np.array(self.errors)\n",
    "        return np.sqrt(np.mean(self.errors**2))\n",
    "    \n",
    "    # 정해진 반복 횟수만큼 앞의 식 2번,4번을 사용해서 P,Q,bu,bd 값을 업데이트하는 함수\n",
    "    def train(self):\n",
    "        # Initializing user-feature and movie-feature matrix\n",
    "        # 행렬을 임의의 값으로 채운다. 여기서는 평균 0, 표준편차 1/K인 정규분포를 갖는 난수로 초기화한다.\n",
    "        self.P = np.random.normal(scale=1./self.k,size=(self.num_users,self.k)) # K가 커질수록 행렬의 각 원소는 더 많은 잠재요인들과 조합, 이때 초기값의 범위를 너무 넓게 설정하면 모델의 표현 범위가 불필요하게 커져 학습 과정에서 불안정성이 증가 또 너무 작은 초기값을 가지면 학습 속도가 느려지고 모델이 충분히 다양한 방향으로 학습되지 않을 수 있다\n",
    "        self.Q = np.random.normal(scale=1./self.k,size=(self.num_items,self.k))\n",
    "        \n",
    "        #Initializing the bias terms\n",
    "        # bu,bd를 0으로 초기화\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_d = np.zeros(self.num_items)\n",
    "        # 0이 아닌 전체 평점 평균을 b에 저장\n",
    "        self.b = np.mean(self.R[self.R.nonzero()])\n",
    "        \n",
    "        # List of training samples\n",
    "        rows,columns = self.R.nonzero()\n",
    "        # 평점이 있는 요소의 인덱스, 평점을 리스트로 저장\n",
    "        self.samples = [(i,j,self.R[i,j]) for i,j in zip(rows,columns)]\n",
    "        \n",
    "        # SGD for given number of iterations\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            rmse=self.rmse()\n",
    "            training_process.append((i+1,rmse))\n",
    "            if self.verbose:\n",
    "                if (i+1)%10==0:\n",
    "                    print(\"Iteration: %d ; Train RMSE = %.4f\"%(i+1,rmse))\n",
    "        return training_process\n",
    "\n",
    "    # Rating prediction for user i and item j\n",
    "    def get_prediction(self,i,j):\n",
    "        prediction = self.b+self.b_u[i]+self.b_d[j]+self.P[i,:].dot(self.Q[j,:].T)\n",
    "        return prediction\n",
    "    \n",
    "    # SGD to get optimized P and Q matrix\n",
    "    def sgd(self):\n",
    "        for i,j,r in self.samples:\n",
    "            prediction = self.get_prediction(i,j)\n",
    "            e = (r-prediction)\n",
    "            \n",
    "            # 앞의 4번 식을 적용해서 bu,bd 업데이트\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_d[j] += self.alpha * (e - self.beta * self.b_d[j])\n",
    "            \n",
    "            # 앞의 2번 식을 적용해서 P,Q 업데이트\n",
    "            self.P[i,:] += self.alpha*(e*self.Q[j,:] - self.beta*self.P[i,:])\n",
    "            self.Q[j,:] += self.alpha*(e*self.P[i,:] - self.beta*self.Q[j,:])\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T08:09:40.743869Z",
     "start_time": "2024-11-25T08:05:36.884344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 전체 데이터 사용 MF\n",
    "# dataframe 형식을 full matrix로 변환 -> MF 클래스 내부적으로 full matrix(self.R)를 계산에 사용하기 때문\n",
    "R_temp = ratings.pivot(index = 'user_id',columns = 'movie_id',values = 'rating').fillna(0)\n",
    "mf = MF(R_temp,k=30,alpha=0.001,beta=0.02,iterations=100,verbose=True)\n",
    "train_process = mf.train()"
   ],
   "id": "1a2558883ebe4b7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 0.9585\n",
      "Iteration: 20 ; Train RMSE = 0.9373\n",
      "Iteration: 30 ; Train RMSE = 0.9280\n",
      "Iteration: 40 ; Train RMSE = 0.9225\n",
      "Iteration: 50 ; Train RMSE = 0.9183\n",
      "Iteration: 60 ; Train RMSE = 0.9144\n",
      "Iteration: 70 ; Train RMSE = 0.9099\n",
      "Iteration: 80 ; Train RMSE = 0.9037\n",
      "Iteration: 90 ; Train RMSE = 0.8948\n",
      "Iteration: 100 ; Train RMSE = 0.8828\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.4 train/test 분리 MF 알고리즘\n",
    "\n",
    "<br>\n",
    "\n",
    "CF에서는 sklearn의 train_test_split을 사용했는데 여기서는 sklearn의 shuffle을 사용"
   ],
   "id": "2e198f4a12acebb7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T10:11:00.834379Z",
     "start_time": "2024-11-28T10:10:37.413544Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train test 분리\n",
    "from sklearn.utils import shuffle\n",
    "TRAIN_SIZE = 0.75\n",
    "# dataframe 형식으로 되어있는 ratings를 무작위로 섞는다\n",
    "ratings = shuffle(ratings,random_state=1) # random_state : 랜덤 시드\n",
    "\n",
    "# 전체 데이터 중 train 데이터 개수\n",
    "cutoff = int(TRAIN_SIZE*len(ratings))\n",
    "ratings_train = ratings.iloc[:cutoff] # iloc[] : 정수 기반 인덱싱, 행과 열의 숫자 위치로 기반\n",
    "ratings_test = ratings.iloc[cutoff:]"
   ],
   "id": "8fc42fb6a95e965d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "user_id와 item_id를 각각의 인덱스와 매핑하는 user_id_index,item_id_index가 클래스 속성으로 추가된 것.  \n",
    "이것이 필요한 이유는 user_id와 item_id가 내부의 인덱스와 일치하지 않기 때문이다.  \n",
    "user_id와 item_id가 연속값이 아닐 수도 있다 이 경우 데이터는 클래스 내부에서 numpy array인 self.R로 변환되면서 중간이 비어있는 실제 아이디와 R의 인덱스가 일치하지 않게 된다. 왜냐하면 numpy array는 무조건 연속되는 값이 지정되지만 아이디는 그렇지 않기 때문\n",
    "- 연속된 값 ex) 0,1,2 ..."
   ],
   "id": "e041c5ef72443cda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T10:17:12.059382Z",
     "start_time": "2024-11-28T10:13:54.364283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# New MF class for training & testing\n",
    "class NEW_MF():\n",
    "    def __init__(self, ratings, k, alpha, beta, iterations, verbose=True):\n",
    "        self.R = np.array(ratings)\n",
    "        # user_id, item_id를 R의 index와 매핑하기 위한 dictionary\n",
    "        item_id_index = []\n",
    "        index_item_id = []\n",
    "        for i,one_id in enumerate(ratings): # enumerate : (인덱스,값) 형식의 튜플을 생성하여 반환\n",
    "            item_id_index.append([one_id,i])\n",
    "            index_item_id.append([i,one_id])\n",
    "        self.item_id_index = dict(item_id_index)\n",
    "        self.index_item_id = dict(index_item_id)\n",
    "        \n",
    "        user_id_index = []\n",
    "        index_user_id = []\n",
    "        for i,one_id in enumerate(ratings.T):\n",
    "            user_id_index.append([one_id,i])\n",
    "            index_user_id.append([i,one_id])\n",
    "        self.user_id_index = dict(user_id_index)\n",
    "        self.index_user_id = dict(index_user_id)\n",
    "\n",
    "        self.num_users, self.num_items = np.shape(self.R)\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.iterations = iterations\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    # train set의 RMSE 계산\n",
    "    def rmse(self):\n",
    "        xs, ys = self.R.nonzero()\n",
    "        self.predictions = []\n",
    "        self.errors = []\n",
    "        for x, y in zip(xs, ys):\n",
    "            prediction = self.get_prediction(x, y)\n",
    "            self.predictions.append(prediction)\n",
    "            self.errors.append(self.R[x, y] - prediction)\n",
    "        self.predictions = np.array(self.predictions)\n",
    "        self.errors = np.array(self.errors)\n",
    "        return np.sqrt(np.mean(self.errors**2))\n",
    "\n",
    "    # Ratings for user i and item j\n",
    "    def get_prediction(self, i, j):\n",
    "        prediction = self.b + self.b_u[i] + self.b_d[j] + self.P[i, :].dot(self.Q[j, :].T)\n",
    "        return prediction\n",
    "\n",
    "    # SGD to get optimized P and Q matrix\n",
    "    def sgd(self):\n",
    "        for i, j, r in self.samples:\n",
    "            prediction = self.get_prediction(i, j)\n",
    "            e = (r - prediction)\n",
    "\n",
    "            self.b_u[i] += self.alpha * (e - self.beta * self.b_u[i])\n",
    "            self.b_d[j] += self.alpha * (e - self.beta * self.b_d[j])\n",
    "\n",
    "            self.P[i, :] += self.alpha * (e * self.Q[j, :] - self.beta * self.P[i,:])\n",
    "            self.Q[j, :] += self.alpha * (e * self.P[i, :] - self.beta * self.Q[j,:])\n",
    "    \n",
    "    # Test set 선정\n",
    "    def set_test(self,ratings_test):\n",
    "        test_set = []\n",
    "        for i in range(len(ratings_test)):\n",
    "            # ratings_test에서 index 뽑아오기 (0:유저, 1:아이템, 2:평점)\n",
    "            x = self.user_id_index[ratings_test.iloc[i,0]]\n",
    "            y = self.item_id_index[ratings_test.iloc[i,1]]\n",
    "            z = ratings_test.iloc[i,2]\n",
    "            test_set.append([x,y,z])\n",
    "            # 해당 (사용자-아이템-평점)을 R에서 0으로 지운다 -> R을 사용해서 MF모델을 학습을 하기 때문에 test set은 R에서 제거해야 한다\n",
    "            self.R[x,y] = 0\n",
    "        self.test_set = test_set\n",
    "        return test_set\n",
    "\n",
    "    # Test set의 RMSE 계산\n",
    "    def test_rmse(self):\n",
    "        error = 0\n",
    "        for one_set in self.test_set:\n",
    "            predicted = self.get_prediction(one_set[0],one_set[1])\n",
    "            error += pow(one_set[2]-predicted,2)\n",
    "        return np.sqrt(error/len(self.test_set))\n",
    "    \n",
    "    def test(self):\n",
    "        self.P = np.random.normal(scale=1./self.k,size=(self.num_users,self.k))\n",
    "        self.Q = np.random.normal(scale=1./self.k,size=(self.num_items,self.k))\n",
    "        self.b_u = np.zeros(self.num_users)\n",
    "        self.b_d = np.zeros(self.num_items)\n",
    "        self.b = np.mean(self.R[self.R.nonzero()])\n",
    "        \n",
    "        row,columns = self.R.nonzero()\n",
    "        self.samples = [(i,j,self.R[i,j]) for i,j in zip(row,columns)]\n",
    "        training_process = []\n",
    "        for i in range(self.iterations):\n",
    "            np.random.shuffle(self.samples)\n",
    "            self.sgd()\n",
    "            # train set의 rmse\n",
    "            rmse1 = self.rmse()\n",
    "            # test set의 rmse\n",
    "            rmse2 = self.test_rmse()\n",
    "            training_process.append((i+1,rmse1,rmse2))\n",
    "            if self.verbose:\n",
    "                if (i+1)%10 == 0:\n",
    "                    print(\"Iteration: %d ; Train RMSE = %.4f ; Test RMSE = %.4f\"%(i+1,rmse1,rmse2))\n",
    "        return training_process\n",
    "    \n",
    "    # 주어진 user_id와 item_id에 대한 예측치\n",
    "    def get_one_prediction(self,user_id,item_id):\n",
    "        return self.get_prediction(self.user_id_index[user_id],self.item_id_index[item_id])\n",
    "    \n",
    "    # 앞의 식 3번에 따라 모든 사용자의 모든 아이템에 대한 예측치(full matrix)를 계산해서 돌려준다\n",
    "    def full_prediction(self):\n",
    "        return self.b + self.b_u[:,np.newaxis] +self.b_d[np.newaxis,:] + self.P.dot(self.Q.T) # np.newaxis : 배열의 차원을 확장\n",
    "    \n",
    "# Testing MF RMSE\n",
    "R_temp = ratings.pivot(index='user_id',columns='movie_id',values='rating').fillna(0)\n",
    "mf = NEW_MF(R_temp,k=30,alpha=0.001,beta=0.02,iterations=100,verbose=True)\n",
    "test_set = mf.set_test(ratings_test)\n",
    "result = mf.test()\n",
    "\n",
    "# 전체 예측치\n",
    "print(mf.full_prediction())\n",
    "# 개별 예측치\n",
    "print(mf.get_prediction(1,2))"
   ],
   "id": "a95ec72fa859a711",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 0.9659 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9410 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9298 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9230 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9183 ; Test RMSE = 0.9496\n",
      "Iteration: 60 ; Train RMSE = 0.9144 ; Test RMSE = 0.9478\n",
      "Iteration: 70 ; Train RMSE = 0.9106 ; Test RMSE = 0.9463\n",
      "Iteration: 80 ; Train RMSE = 0.9065 ; Test RMSE = 0.9450\n",
      "Iteration: 90 ; Train RMSE = 0.9015 ; Test RMSE = 0.9435\n",
      "Iteration: 100 ; Train RMSE = 0.8950 ; Test RMSE = 0.9416\n",
      "[[3.8310796  3.40450863 3.07912732 ... 3.33954345 3.45849851 3.44459865]\n",
      " [3.92112687 3.48917323 3.17945204 ... 3.40953848 3.54356612 3.53594085]\n",
      " [3.31286303 2.89316299 2.5668828  ... 2.80355739 2.93384472 2.93126323]\n",
      " ...\n",
      " [4.2163712  3.78120911 3.42653732 ... 3.70915111 3.83387884 3.82616531]\n",
      " [4.33631707 3.90810648 3.5659206  ... 3.83990981 3.95627306 3.95033172]\n",
      " [3.80204685 3.36351619 3.05286902 ... 3.26753666 3.42639187 3.42049549]]\n",
      "3.1794520439731286\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 연습문제\n",
    "\n",
    "4.1 train/test set을 분리하는 방법을 shuffle() 대신에 앞장에서 사용한 train_test_split()을 사용해서 분리하도록 수정하고 실행해 보세요."
   ],
   "id": "4a14903ef982a27f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T10:21:52.492379Z",
     "start_time": "2024-11-28T10:21:52.470314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ratings_train_1 , ratings_test_1 = train_test_split(ratings,test_size=0.25,random_state=42)\n",
    "\n",
    "print(ratings_test.head())\n",
    "print(ratings_test_1.head())"
   ],
   "id": "fda36d6d13099a52",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       user_id  movie_id  rating\n",
      "53670      345       715       4\n",
      "77110       92       998       2\n",
      "69323      934       195       4\n",
      "85968      586       423       2\n",
      "30243      336       383       1\n",
      "       user_id  movie_id  rating\n",
      "3141        83       364       1\n",
      "95896      451       990       3\n",
      "37382      344       174       5\n",
      "73045      798       377       3\n",
      "76085      847        93       1\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T10:34:38.087870Z",
     "start_time": "2024-11-28T10:31:20.404597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Testing MF RMSE\n",
    "# 기존의 R_temp는 전체 데이터를 기반으로 생성 따라서 split으로 분리한 의미가없다\n",
    "# 연습문제의 R_temp는 훈련 데이터로만 만들어져야한다\n",
    "R_temp = ratings_train_1.pivot(index='user_id',columns='movie_id',values='rating').fillna(0)\n",
    "mf = NEW_MF(R_temp,k=30,alpha=0.001,beta=0.02,iterations=100,verbose=True)\n",
    "\n",
    "# test 데이터의 인덱스가 train데이터에 없기 때문에 오류발생\n",
    "# 따라서 test 데이터에서 train 데이터 안에 없는 인덱스 삭제\n",
    "ratings_test_1_filtered = ratings_test_1[\n",
    "    (ratings_test_1['user_id'].isin(mf.user_id_index)) & \n",
    "    (ratings_test_1['movie_id'].isin(mf.item_id_index))\n",
    "]\n",
    "test_set = mf.set_test(ratings_test_1_filtered)\n",
    "result = mf.test()\n",
    "\n",
    "# 전체 예측치\n",
    "print(mf.full_prediction())\n",
    "# 개별 예측치\n",
    "print(mf.get_prediction(1,2))"
   ],
   "id": "1a765a17a9f79644",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 ; Train RMSE = 0.9698 ; Test RMSE = 0.9707\n",
      "Iteration: 20 ; Train RMSE = 0.9447 ; Test RMSE = 0.9518\n",
      "Iteration: 30 ; Train RMSE = 0.9334 ; Test RMSE = 0.9442\n",
      "Iteration: 40 ; Train RMSE = 0.9265 ; Test RMSE = 0.9402\n",
      "Iteration: 50 ; Train RMSE = 0.9217 ; Test RMSE = 0.9377\n",
      "Iteration: 60 ; Train RMSE = 0.9177 ; Test RMSE = 0.9361\n",
      "Iteration: 70 ; Train RMSE = 0.9139 ; Test RMSE = 0.9348\n",
      "Iteration: 80 ; Train RMSE = 0.9098 ; Test RMSE = 0.9336\n",
      "Iteration: 90 ; Train RMSE = 0.9046 ; Test RMSE = 0.9321\n",
      "Iteration: 100 ; Train RMSE = 0.8980 ; Test RMSE = 0.9303\n",
      "[[3.96880232 3.31234044 3.10347069 ... 3.35560176 3.50258605 3.4636526 ]\n",
      " [3.92621897 3.31275063 3.09081387 ... 3.35640552 3.4909482  3.46483497]\n",
      " [3.38637729 2.7529703  2.53667333 ... 2.79457976 2.92862808 2.88415276]\n",
      " ...\n",
      " [4.29549143 3.69639873 3.45241815 ... 3.72400048 3.83982996 3.80225177]\n",
      " [4.38037387 3.78636196 3.55720907 ... 3.80455097 3.93357401 3.91491746]\n",
      " [3.7301589  3.19308851 2.91021176 ... 3.19888265 3.30772486 3.28977263]]\n",
      "3.090813869792579\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.5 MF의 최적 파라미터 찾기\n",
    "\n",
    "K(잠재요인),iterations,$\\alpha$,$\\beta$ 와 같은 다른 파라미터도 정확도에 영향을 미친다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "K가 지나치게 커지면 train set에 지나치게 맞춰지는 과적합(over-fitting)이 발생한다. 그래서 K가 커짐에 따라 test set에 대한 정확도는 증가하다가 어느 지점 이후에는 오히려 감소하는 모습을 보인다.  \n",
    "반복횟수 iterations도 마찬가지.  \n",
    "\n",
    "<br>\n",
    "\n",
    "최적의 K와 iterations를 찾는 예\n",
    "1. 최적의 K가 대략 어떤 수인지 확인하기 위해 50부터 260까지 넓은 범위에 대해서 10 간격으로 정확도(RMSE)를 계산한다.\n",
    "2. 최적의 RMSE를 보이는 K를 확인한 후 이 숫자 전후 +-10의 K에 대해서 1의 간격으로 다시 한번 RMSE를 계산해서 최적의 K값을 찾는다.\n",
    "3. iterations는 학습과정에서 충분히 큰 숫자를 주어서 RMSE가 어떻게 변화하는지 관찰해서 주어진 K에 대해서 최적의 iterations값을 구한다.\n",
    "\n",
    "정확도는 실행할때마다 달라지기 때문에 보통 코드 전체를 여러 번 실행해서 결과의 평균을 취하는 것이 보통이다.\n",
    "\n"
   ],
   "id": "c8ddef650adc7cdb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T15:05:36.926960Z",
     "start_time": "2024-11-28T11:09:01.758041Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 50\n",
      "Iteration: 10 ; Train RMSE = 0.9661 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9414 ; Test RMSE = 0.9645\n",
      "Iteration: 30 ; Train RMSE = 0.9305 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9241 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9198 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9164 ; Test RMSE = 0.9479\n",
      "Iteration: 70 ; Train RMSE = 0.9135 ; Test RMSE = 0.9466\n",
      "Iteration: 80 ; Train RMSE = 0.9105 ; Test RMSE = 0.9455\n",
      "Iteration: 90 ; Train RMSE = 0.9071 ; Test RMSE = 0.9443\n",
      "Iteration: 100 ; Train RMSE = 0.9027 ; Test RMSE = 0.9430\n",
      "Iteration: 110 ; Train RMSE = 0.8968 ; Test RMSE = 0.9412\n",
      "Iteration: 120 ; Train RMSE = 0.8887 ; Test RMSE = 0.9388\n",
      "Iteration: 130 ; Train RMSE = 0.8782 ; Test RMSE = 0.9357\n",
      "Iteration: 140 ; Train RMSE = 0.8652 ; Test RMSE = 0.9321\n",
      "Iteration: 150 ; Train RMSE = 0.8503 ; Test RMSE = 0.9285\n",
      "Iteration: 160 ; Train RMSE = 0.8338 ; Test RMSE = 0.9253\n",
      "Iteration: 170 ; Train RMSE = 0.8160 ; Test RMSE = 0.9225\n",
      "Iteration: 180 ; Train RMSE = 0.7968 ; Test RMSE = 0.9203\n",
      "Iteration: 190 ; Train RMSE = 0.7766 ; Test RMSE = 0.9187\n",
      "Iteration: 200 ; Train RMSE = 0.7554 ; Test RMSE = 0.9179\n",
      "Iteration: 210 ; Train RMSE = 0.7336 ; Test RMSE = 0.9177\n",
      "Iteration: 220 ; Train RMSE = 0.7117 ; Test RMSE = 0.9183\n",
      "Iteration: 230 ; Train RMSE = 0.6899 ; Test RMSE = 0.9196\n",
      "Iteration: 240 ; Train RMSE = 0.6686 ; Test RMSE = 0.9214\n",
      "Iteration: 250 ; Train RMSE = 0.6478 ; Test RMSE = 0.9237\n",
      "Iteration: 260 ; Train RMSE = 0.6278 ; Test RMSE = 0.9264\n",
      "Iteration: 270 ; Train RMSE = 0.6086 ; Test RMSE = 0.9294\n",
      "Iteration: 280 ; Train RMSE = 0.5903 ; Test RMSE = 0.9326\n",
      "Iteration: 290 ; Train RMSE = 0.5729 ; Test RMSE = 0.9360\n",
      "Iteration: 300 ; Train RMSE = 0.5564 ; Test RMSE = 0.9394\n",
      "k= 60\n",
      "Iteration: 10 ; Train RMSE = 0.9662 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9415 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9307 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9244 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9201 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9169 ; Test RMSE = 0.9479\n",
      "Iteration: 70 ; Train RMSE = 0.9142 ; Test RMSE = 0.9466\n",
      "Iteration: 80 ; Train RMSE = 0.9114 ; Test RMSE = 0.9455\n",
      "Iteration: 90 ; Train RMSE = 0.9084 ; Test RMSE = 0.9444\n",
      "Iteration: 100 ; Train RMSE = 0.9045 ; Test RMSE = 0.9432\n",
      "Iteration: 110 ; Train RMSE = 0.8993 ; Test RMSE = 0.9414\n",
      "Iteration: 120 ; Train RMSE = 0.8921 ; Test RMSE = 0.9391\n",
      "Iteration: 130 ; Train RMSE = 0.8824 ; Test RMSE = 0.9359\n",
      "Iteration: 140 ; Train RMSE = 0.8701 ; Test RMSE = 0.9322\n",
      "Iteration: 150 ; Train RMSE = 0.8556 ; Test RMSE = 0.9283\n",
      "Iteration: 160 ; Train RMSE = 0.8393 ; Test RMSE = 0.9248\n",
      "Iteration: 170 ; Train RMSE = 0.8217 ; Test RMSE = 0.9217\n",
      "Iteration: 180 ; Train RMSE = 0.8027 ; Test RMSE = 0.9192\n",
      "Iteration: 190 ; Train RMSE = 0.7825 ; Test RMSE = 0.9173\n",
      "Iteration: 200 ; Train RMSE = 0.7613 ; Test RMSE = 0.9162\n",
      "Iteration: 210 ; Train RMSE = 0.7392 ; Test RMSE = 0.9157\n",
      "Iteration: 220 ; Train RMSE = 0.7167 ; Test RMSE = 0.9160\n",
      "Iteration: 230 ; Train RMSE = 0.6941 ; Test RMSE = 0.9170\n",
      "Iteration: 240 ; Train RMSE = 0.6716 ; Test RMSE = 0.9186\n",
      "Iteration: 250 ; Train RMSE = 0.6495 ; Test RMSE = 0.9207\n",
      "Iteration: 260 ; Train RMSE = 0.6280 ; Test RMSE = 0.9232\n",
      "Iteration: 270 ; Train RMSE = 0.6073 ; Test RMSE = 0.9260\n",
      "Iteration: 280 ; Train RMSE = 0.5873 ; Test RMSE = 0.9291\n",
      "Iteration: 290 ; Train RMSE = 0.5683 ; Test RMSE = 0.9322\n",
      "Iteration: 300 ; Train RMSE = 0.5501 ; Test RMSE = 0.9355\n",
      "k= 70\n",
      "Iteration: 10 ; Train RMSE = 0.9662 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9416 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9308 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9245 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9204 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9172 ; Test RMSE = 0.9479\n",
      "Iteration: 70 ; Train RMSE = 0.9146 ; Test RMSE = 0.9466\n",
      "Iteration: 80 ; Train RMSE = 0.9120 ; Test RMSE = 0.9455\n",
      "Iteration: 90 ; Train RMSE = 0.9091 ; Test RMSE = 0.9444\n",
      "Iteration: 100 ; Train RMSE = 0.9055 ; Test RMSE = 0.9432\n",
      "Iteration: 110 ; Train RMSE = 0.9006 ; Test RMSE = 0.9416\n",
      "Iteration: 120 ; Train RMSE = 0.8938 ; Test RMSE = 0.9393\n",
      "Iteration: 130 ; Train RMSE = 0.8847 ; Test RMSE = 0.9362\n",
      "Iteration: 140 ; Train RMSE = 0.8730 ; Test RMSE = 0.9324\n",
      "Iteration: 150 ; Train RMSE = 0.8590 ; Test RMSE = 0.9284\n",
      "Iteration: 160 ; Train RMSE = 0.8433 ; Test RMSE = 0.9245\n",
      "Iteration: 170 ; Train RMSE = 0.8262 ; Test RMSE = 0.9211\n",
      "Iteration: 180 ; Train RMSE = 0.8076 ; Test RMSE = 0.9182\n",
      "Iteration: 190 ; Train RMSE = 0.7877 ; Test RMSE = 0.9160\n",
      "Iteration: 200 ; Train RMSE = 0.7667 ; Test RMSE = 0.9144\n",
      "Iteration: 210 ; Train RMSE = 0.7446 ; Test RMSE = 0.9134\n",
      "Iteration: 220 ; Train RMSE = 0.7219 ; Test RMSE = 0.9131\n",
      "Iteration: 230 ; Train RMSE = 0.6989 ; Test RMSE = 0.9136\n",
      "Iteration: 240 ; Train RMSE = 0.6758 ; Test RMSE = 0.9146\n",
      "Iteration: 250 ; Train RMSE = 0.6530 ; Test RMSE = 0.9161\n",
      "Iteration: 260 ; Train RMSE = 0.6306 ; Test RMSE = 0.9181\n",
      "Iteration: 270 ; Train RMSE = 0.6088 ; Test RMSE = 0.9204\n",
      "Iteration: 280 ; Train RMSE = 0.5878 ; Test RMSE = 0.9230\n",
      "Iteration: 290 ; Train RMSE = 0.5676 ; Test RMSE = 0.9257\n",
      "Iteration: 300 ; Train RMSE = 0.5483 ; Test RMSE = 0.9285\n",
      "k= 80\n",
      "Iteration: 10 ; Train RMSE = 0.9663 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9417 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9309 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9247 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9205 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9175 ; Test RMSE = 0.9479\n",
      "Iteration: 70 ; Train RMSE = 0.9149 ; Test RMSE = 0.9466\n",
      "Iteration: 80 ; Train RMSE = 0.9125 ; Test RMSE = 0.9456\n",
      "Iteration: 90 ; Train RMSE = 0.9097 ; Test RMSE = 0.9446\n",
      "Iteration: 100 ; Train RMSE = 0.9064 ; Test RMSE = 0.9434\n",
      "Iteration: 110 ; Train RMSE = 0.9018 ; Test RMSE = 0.9418\n",
      "Iteration: 120 ; Train RMSE = 0.8955 ; Test RMSE = 0.9396\n",
      "Iteration: 130 ; Train RMSE = 0.8870 ; Test RMSE = 0.9367\n",
      "Iteration: 140 ; Train RMSE = 0.8761 ; Test RMSE = 0.9332\n",
      "Iteration: 150 ; Train RMSE = 0.8632 ; Test RMSE = 0.9294\n",
      "Iteration: 160 ; Train RMSE = 0.8487 ; Test RMSE = 0.9259\n",
      "Iteration: 170 ; Train RMSE = 0.8328 ; Test RMSE = 0.9226\n",
      "Iteration: 180 ; Train RMSE = 0.8153 ; Test RMSE = 0.9198\n",
      "Iteration: 190 ; Train RMSE = 0.7962 ; Test RMSE = 0.9173\n",
      "Iteration: 200 ; Train RMSE = 0.7756 ; Test RMSE = 0.9154\n",
      "Iteration: 210 ; Train RMSE = 0.7538 ; Test RMSE = 0.9141\n",
      "Iteration: 220 ; Train RMSE = 0.7312 ; Test RMSE = 0.9135\n",
      "Iteration: 230 ; Train RMSE = 0.7079 ; Test RMSE = 0.9135\n",
      "Iteration: 240 ; Train RMSE = 0.6845 ; Test RMSE = 0.9141\n",
      "Iteration: 250 ; Train RMSE = 0.6611 ; Test RMSE = 0.9153\n",
      "Iteration: 260 ; Train RMSE = 0.6380 ; Test RMSE = 0.9169\n",
      "Iteration: 270 ; Train RMSE = 0.6154 ; Test RMSE = 0.9189\n",
      "Iteration: 280 ; Train RMSE = 0.5935 ; Test RMSE = 0.9211\n",
      "Iteration: 290 ; Train RMSE = 0.5724 ; Test RMSE = 0.9235\n",
      "Iteration: 300 ; Train RMSE = 0.5521 ; Test RMSE = 0.9261\n",
      "k= 90\n",
      "Iteration: 10 ; Train RMSE = 0.9663 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9417 ; Test RMSE = 0.9645\n",
      "Iteration: 30 ; Train RMSE = 0.9310 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9248 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9207 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9177 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9152 ; Test RMSE = 0.9467\n",
      "Iteration: 80 ; Train RMSE = 0.9129 ; Test RMSE = 0.9456\n",
      "Iteration: 90 ; Train RMSE = 0.9103 ; Test RMSE = 0.9447\n",
      "Iteration: 100 ; Train RMSE = 0.9071 ; Test RMSE = 0.9435\n",
      "Iteration: 110 ; Train RMSE = 0.9029 ; Test RMSE = 0.9420\n",
      "Iteration: 120 ; Train RMSE = 0.8970 ; Test RMSE = 0.9399\n",
      "Iteration: 130 ; Train RMSE = 0.8889 ; Test RMSE = 0.9371\n",
      "Iteration: 140 ; Train RMSE = 0.8785 ; Test RMSE = 0.9336\n",
      "Iteration: 150 ; Train RMSE = 0.8659 ; Test RMSE = 0.9298\n",
      "Iteration: 160 ; Train RMSE = 0.8517 ; Test RMSE = 0.9262\n",
      "Iteration: 170 ; Train RMSE = 0.8360 ; Test RMSE = 0.9229\n",
      "Iteration: 180 ; Train RMSE = 0.8189 ; Test RMSE = 0.9200\n",
      "Iteration: 190 ; Train RMSE = 0.8002 ; Test RMSE = 0.9176\n",
      "Iteration: 200 ; Train RMSE = 0.7801 ; Test RMSE = 0.9157\n",
      "Iteration: 210 ; Train RMSE = 0.7587 ; Test RMSE = 0.9143\n",
      "Iteration: 220 ; Train RMSE = 0.7362 ; Test RMSE = 0.9136\n",
      "Iteration: 230 ; Train RMSE = 0.7131 ; Test RMSE = 0.9135\n",
      "Iteration: 240 ; Train RMSE = 0.6896 ; Test RMSE = 0.9140\n",
      "Iteration: 250 ; Train RMSE = 0.6660 ; Test RMSE = 0.9150\n",
      "Iteration: 260 ; Train RMSE = 0.6427 ; Test RMSE = 0.9165\n",
      "Iteration: 270 ; Train RMSE = 0.6197 ; Test RMSE = 0.9184\n",
      "Iteration: 280 ; Train RMSE = 0.5973 ; Test RMSE = 0.9206\n",
      "Iteration: 290 ; Train RMSE = 0.5756 ; Test RMSE = 0.9229\n",
      "Iteration: 300 ; Train RMSE = 0.5546 ; Test RMSE = 0.9254\n",
      "k= 100\n",
      "Iteration: 10 ; Train RMSE = 0.9663 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9418 ; Test RMSE = 0.9645\n",
      "Iteration: 30 ; Train RMSE = 0.9310 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9249 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9208 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9179 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9155 ; Test RMSE = 0.9467\n",
      "Iteration: 80 ; Train RMSE = 0.9132 ; Test RMSE = 0.9458\n",
      "Iteration: 90 ; Train RMSE = 0.9109 ; Test RMSE = 0.9449\n",
      "Iteration: 100 ; Train RMSE = 0.9081 ; Test RMSE = 0.9439\n",
      "Iteration: 110 ; Train RMSE = 0.9043 ; Test RMSE = 0.9427\n",
      "Iteration: 120 ; Train RMSE = 0.8992 ; Test RMSE = 0.9410\n",
      "Iteration: 130 ; Train RMSE = 0.8919 ; Test RMSE = 0.9385\n",
      "Iteration: 140 ; Train RMSE = 0.8822 ; Test RMSE = 0.9353\n",
      "Iteration: 150 ; Train RMSE = 0.8700 ; Test RMSE = 0.9315\n",
      "Iteration: 160 ; Train RMSE = 0.8559 ; Test RMSE = 0.9276\n",
      "Iteration: 170 ; Train RMSE = 0.8402 ; Test RMSE = 0.9240\n",
      "Iteration: 180 ; Train RMSE = 0.8231 ; Test RMSE = 0.9208\n",
      "Iteration: 190 ; Train RMSE = 0.8046 ; Test RMSE = 0.9181\n",
      "Iteration: 200 ; Train RMSE = 0.7847 ; Test RMSE = 0.9159\n",
      "Iteration: 210 ; Train RMSE = 0.7636 ; Test RMSE = 0.9143\n",
      "Iteration: 220 ; Train RMSE = 0.7414 ; Test RMSE = 0.9132\n",
      "Iteration: 230 ; Train RMSE = 0.7184 ; Test RMSE = 0.9128\n",
      "Iteration: 240 ; Train RMSE = 0.6950 ; Test RMSE = 0.9130\n",
      "Iteration: 250 ; Train RMSE = 0.6714 ; Test RMSE = 0.9137\n",
      "Iteration: 260 ; Train RMSE = 0.6480 ; Test RMSE = 0.9148\n",
      "Iteration: 270 ; Train RMSE = 0.6248 ; Test RMSE = 0.9164\n",
      "Iteration: 280 ; Train RMSE = 0.6022 ; Test RMSE = 0.9183\n",
      "Iteration: 290 ; Train RMSE = 0.5802 ; Test RMSE = 0.9204\n",
      "Iteration: 300 ; Train RMSE = 0.5590 ; Test RMSE = 0.9226\n",
      "k= 110\n",
      "Iteration: 10 ; Train RMSE = 0.9663 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9418 ; Test RMSE = 0.9645\n",
      "Iteration: 30 ; Train RMSE = 0.9311 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9249 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9209 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9180 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9157 ; Test RMSE = 0.9467\n",
      "Iteration: 80 ; Train RMSE = 0.9135 ; Test RMSE = 0.9458\n",
      "Iteration: 90 ; Train RMSE = 0.9113 ; Test RMSE = 0.9449\n",
      "Iteration: 100 ; Train RMSE = 0.9087 ; Test RMSE = 0.9440\n",
      "Iteration: 110 ; Train RMSE = 0.9052 ; Test RMSE = 0.9427\n",
      "Iteration: 120 ; Train RMSE = 0.9004 ; Test RMSE = 0.9411\n",
      "Iteration: 130 ; Train RMSE = 0.8936 ; Test RMSE = 0.9387\n",
      "Iteration: 140 ; Train RMSE = 0.8845 ; Test RMSE = 0.9356\n",
      "Iteration: 150 ; Train RMSE = 0.8731 ; Test RMSE = 0.9319\n",
      "Iteration: 160 ; Train RMSE = 0.8598 ; Test RMSE = 0.9281\n",
      "Iteration: 170 ; Train RMSE = 0.8449 ; Test RMSE = 0.9245\n",
      "Iteration: 180 ; Train RMSE = 0.8286 ; Test RMSE = 0.9212\n",
      "Iteration: 190 ; Train RMSE = 0.8107 ; Test RMSE = 0.9184\n",
      "Iteration: 200 ; Train RMSE = 0.7914 ; Test RMSE = 0.9159\n",
      "Iteration: 210 ; Train RMSE = 0.7705 ; Test RMSE = 0.9140\n",
      "Iteration: 220 ; Train RMSE = 0.7484 ; Test RMSE = 0.9127\n",
      "Iteration: 230 ; Train RMSE = 0.7252 ; Test RMSE = 0.9120\n",
      "Iteration: 240 ; Train RMSE = 0.7014 ; Test RMSE = 0.9119\n",
      "Iteration: 250 ; Train RMSE = 0.6773 ; Test RMSE = 0.9123\n",
      "Iteration: 260 ; Train RMSE = 0.6531 ; Test RMSE = 0.9132\n",
      "Iteration: 270 ; Train RMSE = 0.6292 ; Test RMSE = 0.9146\n",
      "Iteration: 280 ; Train RMSE = 0.6057 ; Test RMSE = 0.9163\n",
      "Iteration: 290 ; Train RMSE = 0.5828 ; Test RMSE = 0.9183\n",
      "Iteration: 300 ; Train RMSE = 0.5607 ; Test RMSE = 0.9204\n",
      "k= 120\n",
      "Iteration: 10 ; Train RMSE = 0.9663 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9418 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9311 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9250 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9210 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9181 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9158 ; Test RMSE = 0.9467\n",
      "Iteration: 80 ; Train RMSE = 0.9137 ; Test RMSE = 0.9458\n",
      "Iteration: 90 ; Train RMSE = 0.9115 ; Test RMSE = 0.9449\n",
      "Iteration: 100 ; Train RMSE = 0.9090 ; Test RMSE = 0.9440\n",
      "Iteration: 110 ; Train RMSE = 0.9056 ; Test RMSE = 0.9428\n",
      "Iteration: 120 ; Train RMSE = 0.9010 ; Test RMSE = 0.9411\n",
      "Iteration: 130 ; Train RMSE = 0.8944 ; Test RMSE = 0.9388\n",
      "Iteration: 140 ; Train RMSE = 0.8856 ; Test RMSE = 0.9357\n",
      "Iteration: 150 ; Train RMSE = 0.8745 ; Test RMSE = 0.9320\n",
      "Iteration: 160 ; Train RMSE = 0.8615 ; Test RMSE = 0.9283\n",
      "Iteration: 170 ; Train RMSE = 0.8470 ; Test RMSE = 0.9246\n",
      "Iteration: 180 ; Train RMSE = 0.8311 ; Test RMSE = 0.9214\n",
      "Iteration: 190 ; Train RMSE = 0.8136 ; Test RMSE = 0.9185\n",
      "Iteration: 200 ; Train RMSE = 0.7945 ; Test RMSE = 0.9160\n",
      "Iteration: 210 ; Train RMSE = 0.7739 ; Test RMSE = 0.9140\n",
      "Iteration: 220 ; Train RMSE = 0.7520 ; Test RMSE = 0.9126\n",
      "Iteration: 230 ; Train RMSE = 0.7290 ; Test RMSE = 0.9118\n",
      "Iteration: 240 ; Train RMSE = 0.7053 ; Test RMSE = 0.9116\n",
      "Iteration: 250 ; Train RMSE = 0.6812 ; Test RMSE = 0.9119\n",
      "Iteration: 260 ; Train RMSE = 0.6571 ; Test RMSE = 0.9127\n",
      "Iteration: 270 ; Train RMSE = 0.6331 ; Test RMSE = 0.9140\n",
      "Iteration: 280 ; Train RMSE = 0.6094 ; Test RMSE = 0.9155\n",
      "Iteration: 290 ; Train RMSE = 0.5864 ; Test RMSE = 0.9174\n",
      "Iteration: 300 ; Train RMSE = 0.5641 ; Test RMSE = 0.9193\n",
      "k= 130\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9419 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9312 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9251 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9211 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9183 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9160 ; Test RMSE = 0.9468\n",
      "Iteration: 80 ; Train RMSE = 0.9140 ; Test RMSE = 0.9458\n",
      "Iteration: 90 ; Train RMSE = 0.9120 ; Test RMSE = 0.9450\n",
      "Iteration: 100 ; Train RMSE = 0.9097 ; Test RMSE = 0.9441\n",
      "Iteration: 110 ; Train RMSE = 0.9067 ; Test RMSE = 0.9431\n",
      "Iteration: 120 ; Train RMSE = 0.9026 ; Test RMSE = 0.9416\n",
      "Iteration: 130 ; Train RMSE = 0.8967 ; Test RMSE = 0.9395\n",
      "Iteration: 140 ; Train RMSE = 0.8887 ; Test RMSE = 0.9366\n",
      "Iteration: 150 ; Train RMSE = 0.8782 ; Test RMSE = 0.9330\n",
      "Iteration: 160 ; Train RMSE = 0.8656 ; Test RMSE = 0.9291\n",
      "Iteration: 170 ; Train RMSE = 0.8513 ; Test RMSE = 0.9253\n",
      "Iteration: 180 ; Train RMSE = 0.8357 ; Test RMSE = 0.9219\n",
      "Iteration: 190 ; Train RMSE = 0.8186 ; Test RMSE = 0.9189\n",
      "Iteration: 200 ; Train RMSE = 0.8000 ; Test RMSE = 0.9164\n",
      "Iteration: 210 ; Train RMSE = 0.7799 ; Test RMSE = 0.9143\n",
      "Iteration: 220 ; Train RMSE = 0.7584 ; Test RMSE = 0.9128\n",
      "Iteration: 230 ; Train RMSE = 0.7356 ; Test RMSE = 0.9119\n",
      "Iteration: 240 ; Train RMSE = 0.7120 ; Test RMSE = 0.9116\n",
      "Iteration: 250 ; Train RMSE = 0.6878 ; Test RMSE = 0.9118\n",
      "Iteration: 260 ; Train RMSE = 0.6633 ; Test RMSE = 0.9126\n",
      "Iteration: 270 ; Train RMSE = 0.6390 ; Test RMSE = 0.9138\n",
      "Iteration: 280 ; Train RMSE = 0.6149 ; Test RMSE = 0.9154\n",
      "Iteration: 290 ; Train RMSE = 0.5915 ; Test RMSE = 0.9173\n",
      "Iteration: 300 ; Train RMSE = 0.5687 ; Test RMSE = 0.9194\n",
      "k= 140\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9419 ; Test RMSE = 0.9645\n",
      "Iteration: 30 ; Train RMSE = 0.9312 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9251 ; Test RMSE = 0.9524\n",
      "Iteration: 50 ; Train RMSE = 0.9211 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9183 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9161 ; Test RMSE = 0.9467\n",
      "Iteration: 80 ; Train RMSE = 0.9141 ; Test RMSE = 0.9458\n",
      "Iteration: 90 ; Train RMSE = 0.9120 ; Test RMSE = 0.9449\n",
      "Iteration: 100 ; Train RMSE = 0.9097 ; Test RMSE = 0.9440\n",
      "Iteration: 110 ; Train RMSE = 0.9066 ; Test RMSE = 0.9429\n",
      "Iteration: 120 ; Train RMSE = 0.9023 ; Test RMSE = 0.9413\n",
      "Iteration: 130 ; Train RMSE = 0.8963 ; Test RMSE = 0.9390\n",
      "Iteration: 140 ; Train RMSE = 0.8882 ; Test RMSE = 0.9360\n",
      "Iteration: 150 ; Train RMSE = 0.8780 ; Test RMSE = 0.9325\n",
      "Iteration: 160 ; Train RMSE = 0.8659 ; Test RMSE = 0.9288\n",
      "Iteration: 170 ; Train RMSE = 0.8524 ; Test RMSE = 0.9253\n",
      "Iteration: 180 ; Train RMSE = 0.8374 ; Test RMSE = 0.9220\n",
      "Iteration: 190 ; Train RMSE = 0.8208 ; Test RMSE = 0.9189\n",
      "Iteration: 200 ; Train RMSE = 0.8025 ; Test RMSE = 0.9163\n",
      "Iteration: 210 ; Train RMSE = 0.7825 ; Test RMSE = 0.9140\n",
      "Iteration: 220 ; Train RMSE = 0.7611 ; Test RMSE = 0.9124\n",
      "Iteration: 230 ; Train RMSE = 0.7385 ; Test RMSE = 0.9113\n",
      "Iteration: 240 ; Train RMSE = 0.7150 ; Test RMSE = 0.9109\n",
      "Iteration: 250 ; Train RMSE = 0.6910 ; Test RMSE = 0.9111\n",
      "Iteration: 260 ; Train RMSE = 0.6668 ; Test RMSE = 0.9118\n",
      "Iteration: 270 ; Train RMSE = 0.6426 ; Test RMSE = 0.9129\n",
      "Iteration: 280 ; Train RMSE = 0.6187 ; Test RMSE = 0.9144\n",
      "Iteration: 290 ; Train RMSE = 0.5953 ; Test RMSE = 0.9161\n",
      "Iteration: 300 ; Train RMSE = 0.5725 ; Test RMSE = 0.9181\n",
      "k= 150\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9419 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9312 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9251 ; Test RMSE = 0.9524\n",
      "Iteration: 50 ; Train RMSE = 0.9212 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9184 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9161 ; Test RMSE = 0.9468\n",
      "Iteration: 80 ; Train RMSE = 0.9142 ; Test RMSE = 0.9458\n",
      "Iteration: 90 ; Train RMSE = 0.9123 ; Test RMSE = 0.9450\n",
      "Iteration: 100 ; Train RMSE = 0.9100 ; Test RMSE = 0.9442\n",
      "Iteration: 110 ; Train RMSE = 0.9071 ; Test RMSE = 0.9431\n",
      "Iteration: 120 ; Train RMSE = 0.9031 ; Test RMSE = 0.9417\n",
      "Iteration: 130 ; Train RMSE = 0.8975 ; Test RMSE = 0.9396\n",
      "Iteration: 140 ; Train RMSE = 0.8897 ; Test RMSE = 0.9368\n",
      "Iteration: 150 ; Train RMSE = 0.8796 ; Test RMSE = 0.9333\n",
      "Iteration: 160 ; Train RMSE = 0.8676 ; Test RMSE = 0.9296\n",
      "Iteration: 170 ; Train RMSE = 0.8540 ; Test RMSE = 0.9259\n",
      "Iteration: 180 ; Train RMSE = 0.8390 ; Test RMSE = 0.9225\n",
      "Iteration: 190 ; Train RMSE = 0.8225 ; Test RMSE = 0.9195\n",
      "Iteration: 200 ; Train RMSE = 0.8044 ; Test RMSE = 0.9168\n",
      "Iteration: 210 ; Train RMSE = 0.7847 ; Test RMSE = 0.9146\n",
      "Iteration: 220 ; Train RMSE = 0.7635 ; Test RMSE = 0.9129\n",
      "Iteration: 230 ; Train RMSE = 0.7412 ; Test RMSE = 0.9118\n",
      "Iteration: 240 ; Train RMSE = 0.7180 ; Test RMSE = 0.9113\n",
      "Iteration: 250 ; Train RMSE = 0.6942 ; Test RMSE = 0.9114\n",
      "Iteration: 260 ; Train RMSE = 0.6701 ; Test RMSE = 0.9121\n",
      "Iteration: 270 ; Train RMSE = 0.6461 ; Test RMSE = 0.9131\n",
      "Iteration: 280 ; Train RMSE = 0.6222 ; Test RMSE = 0.9146\n",
      "Iteration: 290 ; Train RMSE = 0.5988 ; Test RMSE = 0.9163\n",
      "Iteration: 300 ; Train RMSE = 0.5759 ; Test RMSE = 0.9182\n",
      "k= 160\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9833\n",
      "Iteration: 20 ; Train RMSE = 0.9419 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9213 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9185 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9163 ; Test RMSE = 0.9468\n",
      "Iteration: 80 ; Train RMSE = 0.9144 ; Test RMSE = 0.9459\n",
      "Iteration: 90 ; Train RMSE = 0.9126 ; Test RMSE = 0.9451\n",
      "Iteration: 100 ; Train RMSE = 0.9106 ; Test RMSE = 0.9444\n",
      "Iteration: 110 ; Train RMSE = 0.9081 ; Test RMSE = 0.9436\n",
      "Iteration: 120 ; Train RMSE = 0.9047 ; Test RMSE = 0.9424\n",
      "Iteration: 130 ; Train RMSE = 0.8998 ; Test RMSE = 0.9407\n",
      "Iteration: 140 ; Train RMSE = 0.8929 ; Test RMSE = 0.9383\n",
      "Iteration: 150 ; Train RMSE = 0.8837 ; Test RMSE = 0.9352\n",
      "Iteration: 160 ; Train RMSE = 0.8723 ; Test RMSE = 0.9315\n",
      "Iteration: 170 ; Train RMSE = 0.8589 ; Test RMSE = 0.9277\n",
      "Iteration: 180 ; Train RMSE = 0.8441 ; Test RMSE = 0.9241\n",
      "Iteration: 190 ; Train RMSE = 0.8278 ; Test RMSE = 0.9209\n",
      "Iteration: 200 ; Train RMSE = 0.8099 ; Test RMSE = 0.9181\n",
      "Iteration: 210 ; Train RMSE = 0.7904 ; Test RMSE = 0.9157\n",
      "Iteration: 220 ; Train RMSE = 0.7694 ; Test RMSE = 0.9138\n",
      "Iteration: 230 ; Train RMSE = 0.7471 ; Test RMSE = 0.9125\n",
      "Iteration: 240 ; Train RMSE = 0.7238 ; Test RMSE = 0.9118\n",
      "Iteration: 250 ; Train RMSE = 0.6999 ; Test RMSE = 0.9117\n",
      "Iteration: 260 ; Train RMSE = 0.6757 ; Test RMSE = 0.9121\n",
      "Iteration: 270 ; Train RMSE = 0.6514 ; Test RMSE = 0.9130\n",
      "Iteration: 280 ; Train RMSE = 0.6274 ; Test RMSE = 0.9142\n",
      "Iteration: 290 ; Train RMSE = 0.6037 ; Test RMSE = 0.9158\n",
      "Iteration: 300 ; Train RMSE = 0.5807 ; Test RMSE = 0.9176\n",
      "k= 170\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9419 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9524\n",
      "Iteration: 50 ; Train RMSE = 0.9213 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9185 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9163 ; Test RMSE = 0.9468\n",
      "Iteration: 80 ; Train RMSE = 0.9145 ; Test RMSE = 0.9459\n",
      "Iteration: 90 ; Train RMSE = 0.9126 ; Test RMSE = 0.9451\n",
      "Iteration: 100 ; Train RMSE = 0.9106 ; Test RMSE = 0.9443\n",
      "Iteration: 110 ; Train RMSE = 0.9080 ; Test RMSE = 0.9433\n",
      "Iteration: 120 ; Train RMSE = 0.9043 ; Test RMSE = 0.9420\n",
      "Iteration: 130 ; Train RMSE = 0.8992 ; Test RMSE = 0.9401\n",
      "Iteration: 140 ; Train RMSE = 0.8921 ; Test RMSE = 0.9375\n",
      "Iteration: 150 ; Train RMSE = 0.8829 ; Test RMSE = 0.9342\n",
      "Iteration: 160 ; Train RMSE = 0.8717 ; Test RMSE = 0.9305\n",
      "Iteration: 170 ; Train RMSE = 0.8588 ; Test RMSE = 0.9267\n",
      "Iteration: 180 ; Train RMSE = 0.8446 ; Test RMSE = 0.9232\n",
      "Iteration: 190 ; Train RMSE = 0.8288 ; Test RMSE = 0.9200\n",
      "Iteration: 200 ; Train RMSE = 0.8114 ; Test RMSE = 0.9170\n",
      "Iteration: 210 ; Train RMSE = 0.7925 ; Test RMSE = 0.9145\n",
      "Iteration: 220 ; Train RMSE = 0.7720 ; Test RMSE = 0.9125\n",
      "Iteration: 230 ; Train RMSE = 0.7501 ; Test RMSE = 0.9110\n",
      "Iteration: 240 ; Train RMSE = 0.7273 ; Test RMSE = 0.9102\n",
      "Iteration: 250 ; Train RMSE = 0.7037 ; Test RMSE = 0.9100\n",
      "Iteration: 260 ; Train RMSE = 0.6796 ; Test RMSE = 0.9104\n",
      "Iteration: 270 ; Train RMSE = 0.6555 ; Test RMSE = 0.9113\n",
      "Iteration: 280 ; Train RMSE = 0.6314 ; Test RMSE = 0.9125\n",
      "Iteration: 290 ; Train RMSE = 0.6077 ; Test RMSE = 0.9141\n",
      "Iteration: 300 ; Train RMSE = 0.5844 ; Test RMSE = 0.9160\n",
      "k= 180\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9419 ; Test RMSE = 0.9645\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9252 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9213 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9186 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9164 ; Test RMSE = 0.9468\n",
      "Iteration: 80 ; Train RMSE = 0.9146 ; Test RMSE = 0.9459\n",
      "Iteration: 90 ; Train RMSE = 0.9128 ; Test RMSE = 0.9451\n",
      "Iteration: 100 ; Train RMSE = 0.9108 ; Test RMSE = 0.9443\n",
      "Iteration: 110 ; Train RMSE = 0.9082 ; Test RMSE = 0.9433\n",
      "Iteration: 120 ; Train RMSE = 0.9047 ; Test RMSE = 0.9420\n",
      "Iteration: 130 ; Train RMSE = 0.8998 ; Test RMSE = 0.9401\n",
      "Iteration: 140 ; Train RMSE = 0.8928 ; Test RMSE = 0.9374\n",
      "Iteration: 150 ; Train RMSE = 0.8837 ; Test RMSE = 0.9340\n",
      "Iteration: 160 ; Train RMSE = 0.8727 ; Test RMSE = 0.9303\n",
      "Iteration: 170 ; Train RMSE = 0.8602 ; Test RMSE = 0.9267\n",
      "Iteration: 180 ; Train RMSE = 0.8463 ; Test RMSE = 0.9232\n",
      "Iteration: 190 ; Train RMSE = 0.8309 ; Test RMSE = 0.9201\n",
      "Iteration: 200 ; Train RMSE = 0.8139 ; Test RMSE = 0.9172\n",
      "Iteration: 210 ; Train RMSE = 0.7951 ; Test RMSE = 0.9147\n",
      "Iteration: 220 ; Train RMSE = 0.7748 ; Test RMSE = 0.9125\n",
      "Iteration: 230 ; Train RMSE = 0.7530 ; Test RMSE = 0.9109\n",
      "Iteration: 240 ; Train RMSE = 0.7301 ; Test RMSE = 0.9099\n",
      "Iteration: 250 ; Train RMSE = 0.7064 ; Test RMSE = 0.9094\n",
      "Iteration: 260 ; Train RMSE = 0.6823 ; Test RMSE = 0.9095\n",
      "Iteration: 270 ; Train RMSE = 0.6580 ; Test RMSE = 0.9101\n",
      "Iteration: 280 ; Train RMSE = 0.6338 ; Test RMSE = 0.9111\n",
      "Iteration: 290 ; Train RMSE = 0.6099 ; Test RMSE = 0.9125\n",
      "Iteration: 300 ; Train RMSE = 0.5865 ; Test RMSE = 0.9141\n",
      "k= 190\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9420 ; Test RMSE = 0.9645\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9253 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9214 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9186 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9165 ; Test RMSE = 0.9468\n",
      "Iteration: 80 ; Train RMSE = 0.9147 ; Test RMSE = 0.9459\n",
      "Iteration: 90 ; Train RMSE = 0.9130 ; Test RMSE = 0.9452\n",
      "Iteration: 100 ; Train RMSE = 0.9111 ; Test RMSE = 0.9445\n",
      "Iteration: 110 ; Train RMSE = 0.9089 ; Test RMSE = 0.9436\n",
      "Iteration: 120 ; Train RMSE = 0.9058 ; Test RMSE = 0.9425\n",
      "Iteration: 130 ; Train RMSE = 0.9014 ; Test RMSE = 0.9409\n",
      "Iteration: 140 ; Train RMSE = 0.8951 ; Test RMSE = 0.9386\n",
      "Iteration: 150 ; Train RMSE = 0.8865 ; Test RMSE = 0.9354\n",
      "Iteration: 160 ; Train RMSE = 0.8757 ; Test RMSE = 0.9317\n",
      "Iteration: 170 ; Train RMSE = 0.8630 ; Test RMSE = 0.9278\n",
      "Iteration: 180 ; Train RMSE = 0.8489 ; Test RMSE = 0.9241\n",
      "Iteration: 190 ; Train RMSE = 0.8334 ; Test RMSE = 0.9207\n",
      "Iteration: 200 ; Train RMSE = 0.8164 ; Test RMSE = 0.9178\n",
      "Iteration: 210 ; Train RMSE = 0.7980 ; Test RMSE = 0.9152\n",
      "Iteration: 220 ; Train RMSE = 0.7779 ; Test RMSE = 0.9132\n",
      "Iteration: 230 ; Train RMSE = 0.7565 ; Test RMSE = 0.9117\n",
      "Iteration: 240 ; Train RMSE = 0.7340 ; Test RMSE = 0.9108\n",
      "Iteration: 250 ; Train RMSE = 0.7106 ; Test RMSE = 0.9105\n",
      "Iteration: 260 ; Train RMSE = 0.6867 ; Test RMSE = 0.9108\n",
      "Iteration: 270 ; Train RMSE = 0.6625 ; Test RMSE = 0.9116\n",
      "Iteration: 280 ; Train RMSE = 0.6383 ; Test RMSE = 0.9128\n",
      "Iteration: 290 ; Train RMSE = 0.6143 ; Test RMSE = 0.9143\n",
      "Iteration: 300 ; Train RMSE = 0.5908 ; Test RMSE = 0.9161\n",
      "k= 200\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9420 ; Test RMSE = 0.9645\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9253 ; Test RMSE = 0.9524\n",
      "Iteration: 50 ; Train RMSE = 0.9214 ; Test RMSE = 0.9498\n",
      "Iteration: 60 ; Train RMSE = 0.9186 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9165 ; Test RMSE = 0.9468\n",
      "Iteration: 80 ; Train RMSE = 0.9148 ; Test RMSE = 0.9459\n",
      "Iteration: 90 ; Train RMSE = 0.9131 ; Test RMSE = 0.9452\n",
      "Iteration: 100 ; Train RMSE = 0.9112 ; Test RMSE = 0.9444\n",
      "Iteration: 110 ; Train RMSE = 0.9089 ; Test RMSE = 0.9436\n",
      "Iteration: 120 ; Train RMSE = 0.9058 ; Test RMSE = 0.9425\n",
      "Iteration: 130 ; Train RMSE = 0.9014 ; Test RMSE = 0.9409\n",
      "Iteration: 140 ; Train RMSE = 0.8952 ; Test RMSE = 0.9386\n",
      "Iteration: 150 ; Train RMSE = 0.8869 ; Test RMSE = 0.9355\n",
      "Iteration: 160 ; Train RMSE = 0.8764 ; Test RMSE = 0.9319\n",
      "Iteration: 170 ; Train RMSE = 0.8643 ; Test RMSE = 0.9282\n",
      "Iteration: 180 ; Train RMSE = 0.8507 ; Test RMSE = 0.9247\n",
      "Iteration: 190 ; Train RMSE = 0.8356 ; Test RMSE = 0.9214\n",
      "Iteration: 200 ; Train RMSE = 0.8189 ; Test RMSE = 0.9184\n",
      "Iteration: 210 ; Train RMSE = 0.8005 ; Test RMSE = 0.9157\n",
      "Iteration: 220 ; Train RMSE = 0.7804 ; Test RMSE = 0.9135\n",
      "Iteration: 230 ; Train RMSE = 0.7589 ; Test RMSE = 0.9118\n",
      "Iteration: 240 ; Train RMSE = 0.7361 ; Test RMSE = 0.9106\n",
      "Iteration: 250 ; Train RMSE = 0.7126 ; Test RMSE = 0.9100\n",
      "Iteration: 260 ; Train RMSE = 0.6885 ; Test RMSE = 0.9100\n",
      "Iteration: 270 ; Train RMSE = 0.6642 ; Test RMSE = 0.9106\n",
      "Iteration: 280 ; Train RMSE = 0.6399 ; Test RMSE = 0.9115\n",
      "Iteration: 290 ; Train RMSE = 0.6160 ; Test RMSE = 0.9128\n",
      "Iteration: 300 ; Train RMSE = 0.5924 ; Test RMSE = 0.9143\n",
      "k= 210\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9420 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9253 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9214 ; Test RMSE = 0.9498\n",
      "Iteration: 60 ; Train RMSE = 0.9187 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9166 ; Test RMSE = 0.9468\n",
      "Iteration: 80 ; Train RMSE = 0.9148 ; Test RMSE = 0.9459\n",
      "Iteration: 90 ; Train RMSE = 0.9131 ; Test RMSE = 0.9451\n",
      "Iteration: 100 ; Train RMSE = 0.9113 ; Test RMSE = 0.9444\n",
      "Iteration: 110 ; Train RMSE = 0.9090 ; Test RMSE = 0.9436\n",
      "Iteration: 120 ; Train RMSE = 0.9059 ; Test RMSE = 0.9424\n",
      "Iteration: 130 ; Train RMSE = 0.9015 ; Test RMSE = 0.9407\n",
      "Iteration: 140 ; Train RMSE = 0.8953 ; Test RMSE = 0.9383\n",
      "Iteration: 150 ; Train RMSE = 0.8869 ; Test RMSE = 0.9352\n",
      "Iteration: 160 ; Train RMSE = 0.8764 ; Test RMSE = 0.9315\n",
      "Iteration: 170 ; Train RMSE = 0.8643 ; Test RMSE = 0.9278\n",
      "Iteration: 180 ; Train RMSE = 0.8508 ; Test RMSE = 0.9243\n",
      "Iteration: 190 ; Train RMSE = 0.8361 ; Test RMSE = 0.9211\n",
      "Iteration: 200 ; Train RMSE = 0.8199 ; Test RMSE = 0.9182\n",
      "Iteration: 210 ; Train RMSE = 0.8021 ; Test RMSE = 0.9156\n",
      "Iteration: 220 ; Train RMSE = 0.7826 ; Test RMSE = 0.9134\n",
      "Iteration: 230 ; Train RMSE = 0.7616 ; Test RMSE = 0.9117\n",
      "Iteration: 240 ; Train RMSE = 0.7393 ; Test RMSE = 0.9105\n",
      "Iteration: 250 ; Train RMSE = 0.7160 ; Test RMSE = 0.9099\n",
      "Iteration: 260 ; Train RMSE = 0.6920 ; Test RMSE = 0.9099\n",
      "Iteration: 270 ; Train RMSE = 0.6678 ; Test RMSE = 0.9104\n",
      "Iteration: 280 ; Train RMSE = 0.6435 ; Test RMSE = 0.9113\n",
      "Iteration: 290 ; Train RMSE = 0.6194 ; Test RMSE = 0.9126\n",
      "Iteration: 300 ; Train RMSE = 0.5957 ; Test RMSE = 0.9142\n",
      "k= 220\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9420 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9313 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9253 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9214 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9187 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9166 ; Test RMSE = 0.9468\n",
      "Iteration: 80 ; Train RMSE = 0.9149 ; Test RMSE = 0.9459\n",
      "Iteration: 90 ; Train RMSE = 0.9132 ; Test RMSE = 0.9452\n",
      "Iteration: 100 ; Train RMSE = 0.9115 ; Test RMSE = 0.9444\n",
      "Iteration: 110 ; Train RMSE = 0.9093 ; Test RMSE = 0.9436\n",
      "Iteration: 120 ; Train RMSE = 0.9064 ; Test RMSE = 0.9425\n",
      "Iteration: 130 ; Train RMSE = 0.9023 ; Test RMSE = 0.9410\n",
      "Iteration: 140 ; Train RMSE = 0.8964 ; Test RMSE = 0.9387\n",
      "Iteration: 150 ; Train RMSE = 0.8883 ; Test RMSE = 0.9356\n",
      "Iteration: 160 ; Train RMSE = 0.8780 ; Test RMSE = 0.9319\n",
      "Iteration: 170 ; Train RMSE = 0.8658 ; Test RMSE = 0.9280\n",
      "Iteration: 180 ; Train RMSE = 0.8521 ; Test RMSE = 0.9242\n",
      "Iteration: 190 ; Train RMSE = 0.8371 ; Test RMSE = 0.9207\n",
      "Iteration: 200 ; Train RMSE = 0.8207 ; Test RMSE = 0.9176\n",
      "Iteration: 210 ; Train RMSE = 0.8028 ; Test RMSE = 0.9149\n",
      "Iteration: 220 ; Train RMSE = 0.7833 ; Test RMSE = 0.9126\n",
      "Iteration: 230 ; Train RMSE = 0.7625 ; Test RMSE = 0.9109\n",
      "Iteration: 240 ; Train RMSE = 0.7404 ; Test RMSE = 0.9097\n",
      "Iteration: 250 ; Train RMSE = 0.7174 ; Test RMSE = 0.9090\n",
      "Iteration: 260 ; Train RMSE = 0.6938 ; Test RMSE = 0.9090\n",
      "Iteration: 270 ; Train RMSE = 0.6698 ; Test RMSE = 0.9095\n",
      "Iteration: 280 ; Train RMSE = 0.6457 ; Test RMSE = 0.9105\n",
      "Iteration: 290 ; Train RMSE = 0.6218 ; Test RMSE = 0.9118\n",
      "Iteration: 300 ; Train RMSE = 0.5982 ; Test RMSE = 0.9134\n",
      "k= 230\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9420 ; Test RMSE = 0.9645\n",
      "Iteration: 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9253 ; Test RMSE = 0.9524\n",
      "Iteration: 50 ; Train RMSE = 0.9215 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9187 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9167 ; Test RMSE = 0.9468\n",
      "Iteration: 80 ; Train RMSE = 0.9150 ; Test RMSE = 0.9460\n",
      "Iteration: 90 ; Train RMSE = 0.9133 ; Test RMSE = 0.9452\n",
      "Iteration: 100 ; Train RMSE = 0.9116 ; Test RMSE = 0.9445\n",
      "Iteration: 110 ; Train RMSE = 0.9095 ; Test RMSE = 0.9437\n",
      "Iteration: 120 ; Train RMSE = 0.9067 ; Test RMSE = 0.9427\n",
      "Iteration: 130 ; Train RMSE = 0.9027 ; Test RMSE = 0.9412\n",
      "Iteration: 140 ; Train RMSE = 0.8970 ; Test RMSE = 0.9391\n",
      "Iteration: 150 ; Train RMSE = 0.8892 ; Test RMSE = 0.9362\n",
      "Iteration: 160 ; Train RMSE = 0.8793 ; Test RMSE = 0.9327\n",
      "Iteration: 170 ; Train RMSE = 0.8677 ; Test RMSE = 0.9291\n",
      "Iteration: 180 ; Train RMSE = 0.8548 ; Test RMSE = 0.9255\n",
      "Iteration: 190 ; Train RMSE = 0.8406 ; Test RMSE = 0.9223\n",
      "Iteration: 200 ; Train RMSE = 0.8249 ; Test RMSE = 0.9194\n",
      "Iteration: 210 ; Train RMSE = 0.8076 ; Test RMSE = 0.9167\n",
      "Iteration: 220 ; Train RMSE = 0.7887 ; Test RMSE = 0.9145\n",
      "Iteration: 230 ; Train RMSE = 0.7682 ; Test RMSE = 0.9127\n",
      "Iteration: 240 ; Train RMSE = 0.7463 ; Test RMSE = 0.9114\n",
      "Iteration: 250 ; Train RMSE = 0.7234 ; Test RMSE = 0.9107\n",
      "Iteration: 260 ; Train RMSE = 0.6997 ; Test RMSE = 0.9106\n",
      "Iteration: 270 ; Train RMSE = 0.6756 ; Test RMSE = 0.9110\n",
      "Iteration: 280 ; Train RMSE = 0.6514 ; Test RMSE = 0.9118\n",
      "Iteration: 290 ; Train RMSE = 0.6273 ; Test RMSE = 0.9130\n",
      "Iteration: 300 ; Train RMSE = 0.6035 ; Test RMSE = 0.9145\n",
      "k= 240\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9420 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9253 ; Test RMSE = 0.9524\n",
      "Iteration: 50 ; Train RMSE = 0.9215 ; Test RMSE = 0.9498\n",
      "Iteration: 60 ; Train RMSE = 0.9188 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9167 ; Test RMSE = 0.9469\n",
      "Iteration: 80 ; Train RMSE = 0.9150 ; Test RMSE = 0.9459\n",
      "Iteration: 90 ; Train RMSE = 0.9135 ; Test RMSE = 0.9452\n",
      "Iteration: 100 ; Train RMSE = 0.9118 ; Test RMSE = 0.9445\n",
      "Iteration: 110 ; Train RMSE = 0.9098 ; Test RMSE = 0.9438\n",
      "Iteration: 120 ; Train RMSE = 0.9072 ; Test RMSE = 0.9428\n",
      "Iteration: 130 ; Train RMSE = 0.9035 ; Test RMSE = 0.9415\n",
      "Iteration: 140 ; Train RMSE = 0.8981 ; Test RMSE = 0.9394\n",
      "Iteration: 150 ; Train RMSE = 0.8906 ; Test RMSE = 0.9365\n",
      "Iteration: 160 ; Train RMSE = 0.8810 ; Test RMSE = 0.9330\n",
      "Iteration: 170 ; Train RMSE = 0.8694 ; Test RMSE = 0.9292\n",
      "Iteration: 180 ; Train RMSE = 0.8564 ; Test RMSE = 0.9255\n",
      "Iteration: 190 ; Train RMSE = 0.8421 ; Test RMSE = 0.9220\n",
      "Iteration: 200 ; Train RMSE = 0.8263 ; Test RMSE = 0.9189\n",
      "Iteration: 210 ; Train RMSE = 0.8091 ; Test RMSE = 0.9161\n",
      "Iteration: 220 ; Train RMSE = 0.7902 ; Test RMSE = 0.9138\n",
      "Iteration: 230 ; Train RMSE = 0.7699 ; Test RMSE = 0.9119\n",
      "Iteration: 240 ; Train RMSE = 0.7482 ; Test RMSE = 0.9106\n",
      "Iteration: 250 ; Train RMSE = 0.7255 ; Test RMSE = 0.9099\n",
      "Iteration: 260 ; Train RMSE = 0.7021 ; Test RMSE = 0.9098\n",
      "Iteration: 270 ; Train RMSE = 0.6781 ; Test RMSE = 0.9101\n",
      "Iteration: 280 ; Train RMSE = 0.6539 ; Test RMSE = 0.9110\n",
      "Iteration: 290 ; Train RMSE = 0.6298 ; Test RMSE = 0.9121\n",
      "Iteration: 300 ; Train RMSE = 0.6059 ; Test RMSE = 0.9136\n",
      "k= 250\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9420 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9254 ; Test RMSE = 0.9524\n",
      "Iteration: 50 ; Train RMSE = 0.9215 ; Test RMSE = 0.9498\n",
      "Iteration: 60 ; Train RMSE = 0.9188 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9168 ; Test RMSE = 0.9469\n",
      "Iteration: 80 ; Train RMSE = 0.9151 ; Test RMSE = 0.9459\n",
      "Iteration: 90 ; Train RMSE = 0.9135 ; Test RMSE = 0.9452\n",
      "Iteration: 100 ; Train RMSE = 0.9119 ; Test RMSE = 0.9445\n",
      "Iteration: 110 ; Train RMSE = 0.9100 ; Test RMSE = 0.9438\n",
      "Iteration: 120 ; Train RMSE = 0.9074 ; Test RMSE = 0.9428\n",
      "Iteration: 130 ; Train RMSE = 0.9037 ; Test RMSE = 0.9414\n",
      "Iteration: 140 ; Train RMSE = 0.8984 ; Test RMSE = 0.9393\n",
      "Iteration: 150 ; Train RMSE = 0.8911 ; Test RMSE = 0.9365\n",
      "Iteration: 160 ; Train RMSE = 0.8817 ; Test RMSE = 0.9330\n",
      "Iteration: 170 ; Train RMSE = 0.8705 ; Test RMSE = 0.9293\n",
      "Iteration: 180 ; Train RMSE = 0.8579 ; Test RMSE = 0.9256\n",
      "Iteration: 190 ; Train RMSE = 0.8439 ; Test RMSE = 0.9222\n",
      "Iteration: 200 ; Train RMSE = 0.8285 ; Test RMSE = 0.9191\n",
      "Iteration: 210 ; Train RMSE = 0.8115 ; Test RMSE = 0.9162\n",
      "Iteration: 220 ; Train RMSE = 0.7928 ; Test RMSE = 0.9138\n",
      "Iteration: 230 ; Train RMSE = 0.7727 ; Test RMSE = 0.9119\n",
      "Iteration: 240 ; Train RMSE = 0.7512 ; Test RMSE = 0.9105\n",
      "Iteration: 250 ; Train RMSE = 0.7287 ; Test RMSE = 0.9097\n",
      "Iteration: 260 ; Train RMSE = 0.7054 ; Test RMSE = 0.9095\n",
      "Iteration: 270 ; Train RMSE = 0.6816 ; Test RMSE = 0.9099\n",
      "Iteration: 280 ; Train RMSE = 0.6575 ; Test RMSE = 0.9106\n",
      "Iteration: 290 ; Train RMSE = 0.6334 ; Test RMSE = 0.9118\n",
      "Iteration: 300 ; Train RMSE = 0.6096 ; Test RMSE = 0.9133\n",
      "k= 260\n",
      "Iteration: 10 ; Train RMSE = 0.9664 ; Test RMSE = 0.9834\n",
      "Iteration: 20 ; Train RMSE = 0.9420 ; Test RMSE = 0.9644\n",
      "Iteration: 30 ; Train RMSE = 0.9314 ; Test RMSE = 0.9566\n",
      "Iteration: 40 ; Train RMSE = 0.9254 ; Test RMSE = 0.9523\n",
      "Iteration: 50 ; Train RMSE = 0.9215 ; Test RMSE = 0.9497\n",
      "Iteration: 60 ; Train RMSE = 0.9188 ; Test RMSE = 0.9480\n",
      "Iteration: 70 ; Train RMSE = 0.9168 ; Test RMSE = 0.9468\n",
      "Iteration: 80 ; Train RMSE = 0.9151 ; Test RMSE = 0.9459\n",
      "Iteration: 90 ; Train RMSE = 0.9135 ; Test RMSE = 0.9452\n",
      "Iteration: 100 ; Train RMSE = 0.9119 ; Test RMSE = 0.9445\n",
      "Iteration: 110 ; Train RMSE = 0.9100 ; Test RMSE = 0.9438\n",
      "Iteration: 120 ; Train RMSE = 0.9074 ; Test RMSE = 0.9428\n",
      "Iteration: 130 ; Train RMSE = 0.9037 ; Test RMSE = 0.9414\n",
      "Iteration: 140 ; Train RMSE = 0.8984 ; Test RMSE = 0.9393\n",
      "Iteration: 150 ; Train RMSE = 0.8910 ; Test RMSE = 0.9365\n",
      "Iteration: 160 ; Train RMSE = 0.8815 ; Test RMSE = 0.9330\n",
      "Iteration: 170 ; Train RMSE = 0.8701 ; Test RMSE = 0.9292\n",
      "Iteration: 180 ; Train RMSE = 0.8573 ; Test RMSE = 0.9255\n",
      "Iteration: 190 ; Train RMSE = 0.8432 ; Test RMSE = 0.9220\n",
      "Iteration: 200 ; Train RMSE = 0.8277 ; Test RMSE = 0.9188\n",
      "Iteration: 210 ; Train RMSE = 0.8106 ; Test RMSE = 0.9160\n",
      "Iteration: 220 ; Train RMSE = 0.7921 ; Test RMSE = 0.9136\n",
      "Iteration: 230 ; Train RMSE = 0.7722 ; Test RMSE = 0.9117\n",
      "Iteration: 240 ; Train RMSE = 0.7511 ; Test RMSE = 0.9103\n",
      "Iteration: 250 ; Train RMSE = 0.7289 ; Test RMSE = 0.9096\n",
      "Iteration: 260 ; Train RMSE = 0.7059 ; Test RMSE = 0.9094\n",
      "Iteration: 270 ; Train RMSE = 0.6825 ; Test RMSE = 0.9098\n",
      "Iteration: 280 ; Train RMSE = 0.6587 ; Test RMSE = 0.9106\n",
      "Iteration: 290 ; Train RMSE = 0.6349 ; Test RMSE = 0.9118\n",
      "Iteration: 300 ; Train RMSE = 0.6113 ; Test RMSE = 0.9133\n"
     ]
    }
   ],
   "execution_count": 9,
   "source": [
    "# 최적의 K값 찾기\n",
    "results = []\n",
    "index = []\n",
    "for k in range(50,261,10):\n",
    "    print('k=',k)\n",
    "    R_temp = ratings.pivot(index = 'user_id',columns = 'movie_id',values = 'rating').fillna(0)\n",
    "    mf = NEW_MF(R_temp,k=k,alpha=0.001,beta=0.02,iterations=300,verbose=True)\n",
    "    test_set = mf.set_test(ratings_test)\n",
    "    result = mf.test()\n",
    "    index.append(k)\n",
    "    results.append(result)"
   ],
   "id": "e360ac2fe797512"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T15:16:00.033301Z",
     "start_time": "2024-11-28T15:16:00.020273Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[50, 207, 0.9177019637346026],\n",
       " [60, 212, 0.915730791309124],\n",
       " [70, 218, 0.9131349235774273],\n",
       " [80, 225, 0.9134378907573648],\n",
       " [90, 227, 0.9134451833208723],\n",
       " [100, 233, 0.9127965301708123],\n",
       " [110, 237, 0.9118326341271404],\n",
       " [120, 239, 0.9115513901514052],\n",
       " [130, 241, 0.9115624939925505],\n",
       " [140, 242, 0.9108814595421894],\n",
       " [150, 243, 0.9112997165943104],\n",
       " [160, 246, 0.9116356073875228],\n",
       " [170, 249, 0.9100121406052449],\n",
       " [180, 254, 0.9093696724198735],\n",
       " [190, 251, 0.9105335210993174],\n",
       " [200, 255, 0.909969228062167],\n",
       " [210, 254, 0.909814855397133],\n",
       " [220, 256, 0.9089586079500345],\n",
       " [230, 257, 0.9105782080513936],\n",
       " [240, 257, 0.9097666783749361],\n",
       " [250, 259, 0.9095135459130043],\n",
       " [260, 258, 0.9094327543399708]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11,
   "source": [
    "# 최적의 iterations값 찾기\n",
    "summary = []\n",
    "for i in range(len(results)):\n",
    "    RMSE = []\n",
    "    for result in results[i]:\n",
    "        RMSE.append(result[2])\n",
    "    min = np.min(RMSE)\n",
    "    j = RMSE.index(min) # min이 RMSE 리스트에서 처음으로 나타나는 위치를 j에 저장\n",
    "    summary.append([index[i],j+1,RMSE[j]])\n",
    "\n",
    "summary"
   ],
   "id": "bbef7149e741d41b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T15:17:48.625929Z",
     "start_time": "2024-11-28T15:17:46.069202Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "execution_count": 12,
   "source": [
    "# 그래프로 그리기\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(index,[x[2] for x in summary])\n",
    "plt.ylim(0.89,0.94)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ],
   "id": "cb883ed75e20d1e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "숫자를 보면 k = 220에서 RMSE 0.908~이 가장 좋은 결과 이지만 k=200 이후로는 사실상 큰 변화가 없다.  \n",
    "그래서 k=200을 최적의 k로 간주한다. 그리고 대략 iterations=250 정도에서 최적의 결과를 보여준다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "K, $\\alpha$, $\\beta$ 와 같은 파라미터 각각에 대해서 최적값을 구했다 하더라도 이들을 단순히 조합한 것이 최적이라는 보장은 없다.  \n",
    "그래서 보통은 파라미터 1의 최적값을 구해서 파라미터 1을 최적으로 고정한 다음에, 파라미터 2의 최적값을 구한다.  \n",
    "모든 파라미터에 대해서 돌아가면서 한 번이나 두 번 정도씩만 최적값을 구해도 대략의 최적값 조합을 얻을 수 있다."
   ],
   "id": "1b704916cb508cd7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7bc1c2a56d4576e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
